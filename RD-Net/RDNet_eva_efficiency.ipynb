{"cells":[{"cell_type":"markdown","source":["## Prepare tools"],"metadata":{"id":"GC_1XO9YRvNo"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"T0rWiucdSITx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712316148283,"user_tz":-480,"elapsed":21530,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"1c283518-9316-4087-a81f-d8f0d3724d79"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aJYV5v6v3vp4","executionInfo":{"status":"ok","timestamp":1712316155233,"user_tz":-480,"elapsed":6953,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}}},"outputs":[],"source":["import torch\n","from torch.autograd import Variable\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.nn.init\n","from scipy.io import loadmat\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UOvELr4OAHKX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712316155234,"user_tz":-480,"elapsed":17,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"b3f7e823-c852-4e2d-8afd-632433bf72d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of GPU:  1 <class 'torch.device'>\n","total GPU memory:  15835660288  memory reserved:  0 memory allocated:  0\n"]}],"source":["torch.cuda.is_available()\n","\n","n_gpu = torch.cuda.device_count()\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","t = torch.cuda.get_device_properties(0).total_memory\n","r = torch.cuda.memory_reserved(0)\n","a = torch.cuda.memory_allocated(0)\n","f = r-a  # free inside reserved\n","\n","print(\"Number of GPU: \", n_gpu, type(device))\n","print(\"total GPU memory: \", t, \" memory reserved: \", r, \"memory allocated: \", a)"]},{"cell_type":"markdown","metadata":{"id":"x8JAZP3DpHxL"},"source":["## Full system implementation"]},{"cell_type":"markdown","source":["### Data loader"],"metadata":{"id":"U5eq1GBd9d7w"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"Z_ani8bi4bhF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712317313776,"user_tz":-480,"elapsed":21424,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"4c240655-f79b-4c24-c46f-fcdad3c8b810"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 45000 .mat files in ./drive/MyDrive/AcFace_AE/RD-Net/Dataset/samples_all\n","Data loader setup complete.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["class AudioFaceDataset(Dataset):\n","    def __init__(self, data_dir, split='train', transform=None, target_transform=None):\n","        self.data_dir = data_dir\n","        self.split = split\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.all_labels = self.get_all_label_df()  # Get all labels without splitting\n","        self.labels = self.split_labels()  # Split the labels according to the specified split\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels.iloc[idx]\n","        label = row[\"label\"]\n","        path = row[\"path\"]\n","        data = self.read_mat_cnn(path)\n","        if self.transform:\n","            data = self.transform(data)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        identifier = path\n","\n","        return data, label, identifier\n","\n","    @staticmethod\n","    def read_mat_cnn(file):\n","        data = loadmat(file)[\"mat_concat\"]\n","        data_tmp = np.expand_dims(data, axis=0)\n","        return data_tmp.astype(np.float32)\n","\n","    def list_all_mat_files(self):\n","        all_files = [str(x.absolute()) for x in Path(self.data_dir).glob(\"**/*.mat\")]\n","        if len(all_files) < 45000:\n","            raise ValueError('Dataset has not been fully synced!')\n","        else:\n","            print(f\"Found {len(all_files)} .mat files in {self.data_dir}\")\n","        return all_files\n","\n","    def convert_path_to_label(self, path_str):\n","        label_start_idx = path_str.rfind('.mat')\n","        face_label = path_str[label_start_idx-3]\n","        mask_label = path_str[label_start_idx-2]\n","        dist_label = path_str[label_start_idx-1]\n","        return \"_\".join([face_label, dist_label, mask_label])\n","\n","    def get_all_label_df(self):\n","        label_dict = {}\n","        for file in self.list_all_mat_files():\n","            label = self.convert_path_to_label(file)\n","            label_dict[file] = label\n","\n","        label_df = pd.DataFrame.from_dict(label_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"path\", 0: \"label\"})\n","        return label_df\n","\n","    def split_labels(self):\n","        all_labels_shuffled = self.all_labels.sample(frac=1).reset_index(drop=True)  # Ensure reproducibility with random_state\n","        if self.split == 'train':\n","            return all_labels_shuffled.sample(frac=0.8)  # Use all data for training\n","        elif self.split == 'test':\n","            return all_labels_shuffled.sample(frac=0.05)  # Use 20% of the data for testing\n","        else:\n","            raise ValueError(\"Split must be 'train' or 'test'.\")\n","\n","data_dir = './drive/MyDrive/AcFace_AE/RD-Net/Dataset/samples_all'\n","data_test = AudioFaceDataset(data_dir, split='test')\n","\n","batch_size = 64  # Specify your batch size\n","data_test_loader = DataLoader(dataset=data_test,\n","                              batch_size=batch_size,\n","                              shuffle=True,  # Typically, we don't need to shuffle the test data\n","                              num_workers=8)\n","\n","print(\"Data loader setup complete.\")"]},{"cell_type":"markdown","source":["### Setup model"],"metadata":{"id":"egpfoxoKUIVG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        if self.downsample:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class RDNet(nn.Module):\n","    def __init__(self, num_face=2, num_dist=2, num_mask=2):\n","        super(RDNet, self).__init__()\n","\n","        self.in_channels = 64\n","        self.conv1 = nn.Conv2d(1, self.in_channels, kernel_size=3, stride=2, padding=1)\n","        self.bn1 = nn.BatchNorm2d(self.in_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        # Adding more depth with Residual Blocks\n","        self.layer1 = self._make_layer(128, stride=2)\n","        self.layer2 = self._make_layer(256, stride=2)\n","        self.layer3 = self._make_layer(512, stride=2)\n","        self.drop = nn.Dropout(p=0.3)\n","\n","        self.adaptivePool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # Increase model capacity in fully connected layers\n","        self.face_fc1 = nn.Linear(512, 2048)\n","        self.face_fc2 = nn.Linear(2048, 2048)\n","        self.face_fc3 = nn.Linear(2048, 1024)\n","        self.face_fc4 = nn.Linear(1024, 1024)\n","        self.face_fc5 = nn.Linear(1024, 1024)\n","        self.face_fc6 = nn.Linear(1024, 1024)\n","        self.face_fc7 = nn.Linear(1024, 1024)\n","        self.face_fc8 = nn.Linear(1024, 512)\n","        self.face_fc9 = nn.Linear(512, 512)\n","        self.face_fc10 = nn.Linear(512, num_face)\n","\n","        self.dist_fc1 = nn.Linear(512 + num_face, 256)\n","        self.dist_fc2 = nn.Linear(256, 256)\n","        self.dist_fc3 = nn.Linear(256, 256)\n","        self.dist_fc4 = nn.Linear(256, 128)\n","        self.dist_fc5 = nn.Linear(128, num_dist)\n","\n","        self.mask_fc1 = nn.Linear(512 + num_face, 256)\n","        self.mask_fc2 = nn.Linear(256, 256)\n","        self.mask_fc3 = nn.Linear(256, 256)\n","        self.mask_fc4 = nn.Linear(256, 128)\n","        self.mask_fc5 = nn.Linear(128, num_mask)\n","\n","    def _make_layer(self, out_channels, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_channels != out_channels:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","        layer = ResidualBlock(self.in_channels, out_channels, stride, downsample)\n","        self.in_channels = out_channels\n","        return layer\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","\n","        x = self.drop(x)\n","        x = self.adaptivePool(x)\n","        x_cnn_output = x.view(x.size(0), -1)\n","\n","        x_face = F.relu(self.face_fc1(x_cnn_output))\n","        x_face = F.relu(self.face_fc2(x_face))\n","        x_face = F.relu(self.face_fc3(x_face))\n","        x_face = F.relu(self.face_fc4(x_face))\n","        x_face = F.relu(self.face_fc5(x_face))\n","        x_face = F.relu(self.face_fc6(x_face))\n","        x_face = F.relu(self.face_fc7(x_face))\n","        x_face = F.relu(self.face_fc8(x_face))\n","        x_face = F.relu(self.face_fc9(x_face))\n","        x_face_output = torch.sigmoid(self.face_fc10(x_face))\n","\n","        x_dist_input = torch.cat((x_cnn_output, x_face_output), 1)\n","        x_dist = F.relu(self.dist_fc1(x_dist_input))\n","        x_dist = F.relu(self.dist_fc2(x_dist))\n","        x_dist = F.relu(self.dist_fc3(x_dist))\n","        x_dist = F.relu(self.dist_fc4(x_dist))\n","        x_dist_output = torch.sigmoid(self.dist_fc5(x_dist))\n","\n","        x_mask_input = torch.cat((x_cnn_output, x_face_output), 1)\n","        x_mask = F.relu(self.mask_fc1(x_mask_input))\n","        x_mask = F.relu(self.mask_fc2(x_mask))\n","        x_mask = F.relu(self.mask_fc3(x_mask))\n","        x_mask = F.relu(self.mask_fc4(x_mask))\n","        x_mask_output = torch.sigmoid(self.mask_fc5(x_mask))\n","\n","        return [x_face_output, x_dist_output, x_mask_output]\n","\n","model = RDNet().to(device)\n","\n","# Calculate total parameters and model size in bytes\n","param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n","buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n","total_size = param_size + buffer_size\n","size_in_mb = total_size / (1024 ** 2)\n","\n","# Print parameters\n","print(f'Total parameters: {sum(p.numel() for p in model.parameters())}')\n","print(f'Model size: {size_in_mb:.3f} MB')\n","\n","# load model\n","model_load_path = './drive/MyDrive/AcFace_AE/RD-Net/Model/model_pretrained.pth'  # The path where your model is saved\n","model.load_state_dict(torch.load(model_load_path))"],"metadata":{"id":"omJhUklEvn2u","executionInfo":{"status":"ok","timestamp":1712317441186,"user_tz":-480,"elapsed":560,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d76f4b8b-2416-4144-f109-a7ac15342131"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters: 17748230\n","Model size: 67.725 MB\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["### Inference test\n","\n","\n"],"metadata":{"id":"MmsGghW9VbWf"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"ZSbOSLMn5Lqe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712318383344,"user_tz":-480,"elapsed":16192,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"023bd2e9-e557-4acc-c8e8-0ccc3fcdd990"},"outputs":[{"output_type":"stream","name":"stdout","text":["Avg Inference Delay: 5.07 ms\n","\n","Avg Accuracy: 96.62 %\n"]}],"source":["import torch\n","import numpy as np\n","import time\n","\n","criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n","model.eval()\n","\n","acc_list = []\n","cost_list = []\n","predictions = []\n","true_labels = []\n","inference_times = []\n","\n","for i, (test_X, test_Y, sample_ids) in enumerate(data_test_loader):\n","    face_Y, dist_Y, mask_Y = [], [], []\n","    for Y_i in test_Y:\n","        underline_idx = Y_i.find(\"_\")\n","        face_Y.append(int(Y_i[underline_idx-1]))\n","        dist_Y.append(int(Y_i[underline_idx+1]))\n","        mask_Y.append(int(Y_i[underline_idx+3]))\n","\n","    X = test_X.to(device)\n","    face_Y = torch.LongTensor(face_Y).to(device)\n","    dist_Y = torch.LongTensor(dist_Y).to(device)\n","    mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","    for j in range(len(X)):\n","        with torch.no_grad():\n","            data_in = X[j].unsqueeze(0)\n","            start_time = time.time()\n","            output = model(data_in)\n","            end_time = time.time()  # End time after inference\n","            inference_times.append(end_time - start_time)  # Calculate inference time for this sample\n","            # print(end_time - start_time)\n","\n","            cost_face = criterion(output[0], face_Y[j].unsqueeze(0))\n","            cost_dist = criterion(output[1], dist_Y[j].unsqueeze(0))\n","            cost_mask = criterion(output[2], mask_Y[j].unsqueeze(0))\n","            cost = cost_face - 0.015 * cost_dist - 0.01 * cost_mask\n","\n","            accuracy = (torch.max(output[0], 1)[1] == face_Y[j]).float().mean().item()\n","\n","            acc_list.append(accuracy)\n","            cost_list.append(cost.item())\n","\n","            predictions.append(torch.max(output[0], 1)[1].cpu().item())\n","            true_labels.append(face_Y[j].cpu().item())\n","\n","            # print(f'Sample {sample_ids[j]} inference time: {time.time() - start_time:.4f} seconds')\n","\n","if acc_list:  # Check if acc_list is not empty\n","    averaged_delay = np.mean(inference_times)\n","    print(f'Avg Inference Delay: {averaged_delay * 1000:.2f} ms')\n","    print('\\nAvg Accuracy: {:2.2f} %'.format(np.mean(acc_list) * 100))\n","else:\n","    raise Exception(\"\\nNo valid accuracy computations were performed.\")"]},{"cell_type":"markdown","source":["## Light-weight implementation"],"metadata":{"id":"cuXF8jB0D2WL"}},{"cell_type":"markdown","source":["### Data loader"],"metadata":{"id":"mJGtVyAlD8lQ"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","from pathlib import Path\n","from scipy.io import loadmat\n","from torch.utils.data import Dataset\n","\n","from sklearn.model_selection import train_test_split\n","\n","class AudioFaceDataset_train(Dataset):\n","    def __init__(self, data_dir, transform=None, target_transform=None):\n","        self.classes = {\n","            \"accept\": 0,\n","            \"reject\": 1\n","        }\n","        self.data_dir = data_dir\n","        self.labels = self.get_label_df()\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels.iloc[idx]\n","        label = row[\"label\"]\n","        path = row[\"path\"]\n","        data = AudioFaceDataset_train.read_mat_cnn(path)\n","        if self.transform:\n","            data = self.transform(data)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return data, label\n","\n","    @staticmethod\n","    def read_mat(file):\n","        data = loadmat(file)[\"fig_mat_flatten\"]\n","        return data.transpose().ravel().astype(np.float32)\n","\n","    @staticmethod\n","    def read_mat_cnn(file):\n","        data = loadmat(file)[\"face_sample\"]\n","        data_tmp = np.expand_dims(data,axis=0)\n","        return data_tmp.astype(np.float32)\n","\n","    def list_all_mat_files(self):\n","        return [str(x.absolute()) for x in Path(self.data_dir).glob(\"**/*.mat\")]\n","\n","    def convert_path_to_label(self, path_str):\n","        label_start_idx = path_str.find('.mat')\n","        face_label = path_str[label_start_idx-3]\n","        mask_label = path_str[label_start_idx-2]\n","        dist_label = path_str[label_start_idx-1]\n","        return face_label + \"_\" + dist_label + \"_\" + mask_label\n","\n","    def get_label_df(self):\n","        label_dict = {}\n","        for file in self.list_all_mat_files():\n","            label_dict[file] = self.convert_path_to_label(file)\n","        label_df = pd.DataFrame.from_dict(label_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"path\",\n","                                                                                                    0: \"label\"})\n","\n","        # Splitting the dataset into training and testing sets\n","        train_df, test_df = train_test_split(label_df, test_size=0.2, random_state=42)  # Adjust test_size as needed\n","        return train_df.reset_index(drop=True)  # Return only the training set\n","\n","class AudioFaceDataset_test(Dataset):\n","    def __init__(self, data_dir, transform=None, target_transform=None):\n","        self.classes = {\n","            \"accept\": 0,\n","            \"reject\": 1\n","        }\n","        self.data_dir = data_dir\n","        self.labels = self.get_label_df()\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels.iloc[idx]\n","        label = row[\"label\"]\n","        path = row[\"path\"]\n","        data = AudioFaceDataset_test.read_mat_cnn(path)\n","        if self.transform:\n","            data = self.transform(data)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return data, label\n","\n","    @staticmethod\n","    def read_mat(file):\n","        data = loadmat(file)[\"fig_mat_flatten\"]\n","        return data.transpose().ravel().astype(np.float32)\n","\n","    @staticmethod\n","    def read_mat_cnn(file):\n","        data = loadmat(file)[\"face_sample\"]\n","        data_tmp = np.expand_dims(data,axis=0)\n","        return data_tmp.astype(np.float32)\n","\n","    def list_all_mat_files(self):\n","        return [str(x.absolute()) for x in Path(self.data_dir).glob(\"**/*.mat\")]\n","\n","    def convert_path_to_label(self, path_str):\n","        label_start_idx = path_str.find('.mat')\n","        face_label = path_str[label_start_idx-3]\n","        mask_label = path_str[label_start_idx-2]\n","        dist_label = path_str[label_start_idx-1]\n","        return face_label + \"_\" + dist_label + \"_\" + mask_label\n","\n","    def get_label_df(self):\n","        label_dict = {}\n","        for file in self.list_all_mat_files():\n","            label_dict[file] = self.convert_path_to_label(file)\n","        label_df = pd.DataFrame.from_dict(label_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"path\",\n","                                                                                                    0: \"label\"})\n","\n","        # Splitting the dataset into training and testing sets\n","        train_df, test_df = train_test_split(label_df, test_size=0.2, random_state=42)  # Adjust test_size as needed\n","        return test_df.reset_index(drop=True)  # Return only the testing set\n","\n","test_data_dir = './drive/MyDrive/AcFace_AE/RD-Net/Dataset/Efficiency/'\n","test_dataset = AudioFaceDataset_test(test_data_dir)\n","\n","batch_size = 64\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)  # No need to shuffle the test data\n"],"metadata":{"id":"JYCIxGwDEImH","executionInfo":{"status":"ok","timestamp":1712317041027,"user_tz":-480,"elapsed":25828,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Setup model"],"metadata":{"id":"bRhRw5JFD-aQ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class RDNet(torch.nn.Module):\n","    def __init__(self, num_face=2, num_dist=2, mask=2):\n","        super().__init__()\n","\n","        # Convolutional Layers\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n","        self.drop = nn.Dropout(p=0.3)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.face_fc1 = nn.Linear(6528, 600)\n","        self.face_fc2 = nn.Linear(600, 300)\n","        self.face_fc3 = nn.Linear(300, num_face)\n","\n","        self.dist_fc1 = nn.Linear(6530, 600)\n","        self.dist_fc2 = nn.Linear(600, 300)\n","        self.dist_fc3 = nn.Linear(300, num_dist)\n","\n","        self.mask_fc1 = nn.Linear(6530, 600)\n","        self.mask_fc2 = nn.Linear(600, 300)\n","        self.mask_fc3 = nn.Linear(300, mask)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        # Convolutional layers with ReLu activations\n","        a = torch.relu(self.conv1(x))\n","        a = torch.relu(self.conv2(a))\n","        # a = self.drop(a)\n","        # a = self.pool(a)\n","        x_cnn_output = a.view((batch_size, -1))\n","\n","        x_face = F.relu(self.face_fc1(x_cnn_output))\n","        x_face = F.relu(self.face_fc2(x_face))\n","        x_face = self.face_fc3(x_face)\n","        x_face_output = torch.sigmoid(x_face)\n","\n","        x_dist_input = torch.cat((x_cnn_output,x_face_output),1)\n","        x_dist = F.relu(self.dist_fc1(x_dist_input))\n","        x_dist = F.relu(self.dist_fc2(x_dist))\n","        x_dist = self.dist_fc3(x_dist)\n","        x_dist_output = torch.sigmoid(x_dist)\n","\n","        x_mask_input = torch.cat((x_cnn_output,x_face_output),1)\n","        x_mask = F.relu(self.mask_fc1(x_mask_input))\n","        x_mask = F.relu(self.mask_fc2(x_mask))\n","        x_mask = self.mask_fc3(x_mask)\n","        x_mask_output = torch.sigmoid(x_mask)\n","\n","        return [x_face_output,x_dist_output,x_mask_output]\n","\n","model = RDNet()\n","model.to(device)\n","\n","param_size = 0\n","for param in model.parameters():\n","    param_size += param.nelement() * param.element_size()\n","buffer_size = 0\n","for buffer in model.buffers():\n","    buffer_size += buffer.nelement() * buffer.element_size()\n","\n","size_all_mb = (param_size + buffer_size) / 1024**2\n","\n","print(f'Total parameters: {sum(p.numel() for p in model.parameters())}')\n","print(f'Model size: {size_all_mb:.3f} MB')\n","\n","model_load_path = './drive/MyDrive/AcFace_AE/RD-Net/Model/efficiency/model_efficiency.pth'  # The path where your model is saved\n","model.load_state_dict(torch.load(model_load_path))"],"metadata":{"id":"Mj8V_fUkEXCe","executionInfo":{"status":"ok","timestamp":1712317076847,"user_tz":-480,"elapsed":5468,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a07a5ccc-3cf3-43bf-c377-a8a00677f71c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters: 12316122\n","Model size: 46.982 MB\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### Inference test"],"metadata":{"id":"-2Nl_GwIEAKw"}},{"cell_type":"code","source":["import time\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","model.eval()\n","\n","test_count = 0\n","acc_list = []\n","cost_list = []\n","inference_times = []\n","\n","for i, (test_X, test_Y) in enumerate(test_loader):\n","    test_count = test_count + 1\n","\n","    face_Y = []\n","    dist_Y = []\n","    mask_Y = []\n","    for Y_i in test_Y:\n","      underline_idx = Y_i.find(\"_\")\n","      face_Y.append(int(Y_i[underline_idx-1]))\n","      dist_Y.append(int(Y_i[underline_idx+1]))\n","      mask_Y.append(int(Y_i[underline_idx+3]))\n","\n","    X = test_X.to(device)\n","    face_Y = torch.LongTensor(face_Y).to(device)\n","    dist_Y = torch.LongTensor(dist_Y).to(device)\n","    mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","    # forward propagation\n","    start_time = time.time()\n","    output = model(X)\n","    end_time = time.time()\n","    inference_times.append(end_time - start_time)\n","    # print(\"inference delay: \", end_time - start_time)\n","    cost_face = criterion(output[0], face_Y)\n","    cost_dist = criterion(output[1], dist_Y)\n","    cost_mask = criterion(output[2], mask_Y)\n","    cost = cost_face - (0.03*cost_dist + 0.02*cost_mask)/2\n","\n","    acc_list.append((torch.max(output[0],dim=1)[1] == face_Y).float().mean().item())\n","    cost_list.append(cost.item())\n","\n","if acc_list:  # Check if acc_list is not empty\n","    averaged_delay = np.mean(inference_times)\n","    print(f'Avg Inference Delay: {averaged_delay * 1000:.2f} ms')\n","    print('\\nAvg Accuracy: {:2.2f} %'.format(np.mean(acc_list) * 100))\n","else:\n","    raise Exception(\"\\nNo valid accuracy computations were performed.\")"],"metadata":{"id":"9ReVvrPnE4JY","executionInfo":{"status":"ok","timestamp":1712317202615,"user_tz":-480,"elapsed":2105,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7961ef40-a0c9-4971-82f3-38bcd2902347"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Avg Inference Delay: 1.14 ms\n","\n","Avg Accuracy: 91.67 %\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
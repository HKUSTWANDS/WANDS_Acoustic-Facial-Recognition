{"cells":[{"cell_type":"markdown","source":["## Prepare tools"],"metadata":{"id":"GC_1XO9YRvNo"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"T0rWiucdSITx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712307324610,"user_tz":-480,"elapsed":2657,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"bac5fd55-b573-4480-d553-37998f392137"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"aJYV5v6v3vp4","executionInfo":{"status":"ok","timestamp":1712307324610,"user_tz":-480,"elapsed":11,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}}},"outputs":[],"source":["import torch\n","from torch.autograd import Variable\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.nn.init\n","from scipy.io import loadmat\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"UOvELr4OAHKX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712307324610,"user_tz":-480,"elapsed":11,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"05164179-4fc0-48d7-f842-b502d7fdb86a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of GPU:  1 <class 'torch.device'>\n","total GPU memory:  15835660288  memory reserved:  0 memory allocated:  0\n"]}],"source":["torch.cuda.is_available()\n","\n","n_gpu = torch.cuda.device_count()\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","t = torch.cuda.get_device_properties(0).total_memory\n","r = torch.cuda.memory_reserved(0)\n","a = torch.cuda.memory_allocated(0)\n","f = r-a  # free inside reserved\n","\n","print(\"Number of GPU: \", n_gpu, type(device))\n","print(\"total GPU memory: \", t, \" memory reserved: \", r, \"memory allocated: \", a)"]},{"cell_type":"markdown","metadata":{"id":"x8JAZP3DpHxL"},"source":["## Setup dataloader"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Z_ani8bi4bhF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712307451569,"user_tz":-480,"elapsed":126969,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"a5b7d1a3-04c6-4bfe-ac1b-74a19d02a89f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 45000 .mat files in ./drive/MyDrive/AcFace_AE/RD-Net/Dataset/samples_all\n","Found 45000 .mat files in ./drive/MyDrive/AcFace_AE/RD-Net/Dataset/samples_all\n","Data loader setup complete.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["class AudioFaceDataset(Dataset):\n","    def __init__(self, data_dir, split='train', transform=None, target_transform=None):\n","        self.data_dir = data_dir\n","        self.split = split\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.all_labels = self.get_all_label_df()  # Get all labels without splitting\n","        self.labels = self.split_labels()  # Split the labels according to the specified split\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels.iloc[idx]\n","        label = row[\"label\"]\n","        path = row[\"path\"]\n","        data = self.read_mat_cnn(path)\n","        if self.transform:\n","            data = self.transform(data)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        identifier = path\n","\n","        return data, label, identifier\n","\n","    @staticmethod\n","    def read_mat_cnn(file):\n","        data = loadmat(file)[\"mat_concat\"]\n","        data_tmp = np.expand_dims(data, axis=0)\n","        return data_tmp.astype(np.float32)\n","\n","    def list_all_mat_files(self):\n","        all_files = [str(x.absolute()) for x in Path(self.data_dir).glob(\"**/*.mat\")]\n","        if len(all_files) < 45000:\n","            raise ValueError('Dataset has not been fully synced!')\n","        else:\n","            print(f\"Found {len(all_files)} .mat files in {self.data_dir}\")\n","        return all_files\n","\n","    def convert_path_to_label(self, path_str):\n","        label_start_idx = path_str.rfind('.mat')\n","        face_label = path_str[label_start_idx-3]\n","        mask_label = path_str[label_start_idx-2]\n","        dist_label = path_str[label_start_idx-1]\n","        return \"_\".join([face_label, dist_label, mask_label])\n","\n","    def get_all_label_df(self):\n","        label_dict = {}\n","        for file in self.list_all_mat_files():\n","            label = self.convert_path_to_label(file)\n","            label_dict[file] = label\n","\n","        label_df = pd.DataFrame.from_dict(label_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"path\", 0: \"label\"})\n","        return label_df\n","\n","    def split_labels(self):\n","        all_labels_shuffled = self.all_labels.sample(frac=1).reset_index(drop=True)  # Ensure reproducibility with random_state\n","        if self.split == 'train':\n","            return all_labels_shuffled.sample(frac=0.8)  # Use all data for training\n","        elif self.split == 'test':\n","            return all_labels_shuffled.sample(frac=0.2)  # Use 20% of the data for testing\n","        else:\n","            raise ValueError(\"Split must be 'train' or 'test'.\")\n","\n","data_dir = './drive/MyDrive/AcFace_AE/RD-Net/Dataset/samples_all'\n","\n","# Creating instances for training and testing\n","data_train = AudioFaceDataset(data_dir, split='train')\n","data_test = AudioFaceDataset(data_dir, split='test')\n","\n","# Setup DataLoader for training\n","batch_size = 128  # Specify your batch size\n","data_train_loader = DataLoader(dataset=data_train,\n","                               batch_size=batch_size,\n","                               shuffle=True,\n","                               num_workers=8)\n","\n","# Setup DataLoader for testing\n","data_test_loader = DataLoader(dataset=data_test,\n","                              batch_size=batch_size,\n","                              shuffle=True,  # Typically, we don't need to shuffle the test data\n","                              num_workers=8)\n","\n","print(\"Data loader setup complete.\")"]},{"cell_type":"markdown","source":["## Setup model"],"metadata":{"id":"egpfoxoKUIVG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        if self.downsample:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class RDNet(nn.Module):\n","    def __init__(self, num_face=2, num_dist=2, num_mask=2):\n","        super(RDNet, self).__init__()\n","\n","        self.in_channels = 64\n","        self.conv1 = nn.Conv2d(1, self.in_channels, kernel_size=3, stride=2, padding=1)\n","        self.bn1 = nn.BatchNorm2d(self.in_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        # Adding more depth with Residual Blocks\n","        self.layer1 = self._make_layer(128, stride=2)\n","        self.layer2 = self._make_layer(256, stride=2)\n","        self.layer3 = self._make_layer(512, stride=2)\n","        self.drop = nn.Dropout(p=0.3)\n","\n","        self.adaptivePool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # Increase model capacity in fully connected layers\n","        self.face_fc1 = nn.Linear(512, 2048)\n","        self.face_fc2 = nn.Linear(2048, 2048)\n","        self.face_fc3 = nn.Linear(2048, 1024)\n","        self.face_fc4 = nn.Linear(1024, 1024)\n","        self.face_fc5 = nn.Linear(1024, 1024)\n","        self.face_fc6 = nn.Linear(1024, 1024)\n","        self.face_fc7 = nn.Linear(1024, 1024)\n","        self.face_fc8 = nn.Linear(1024, 512)\n","        self.face_fc9 = nn.Linear(512, 512)\n","        self.face_fc10 = nn.Linear(512, num_face)\n","\n","        self.dist_fc1 = nn.Linear(512 + num_face, 256)\n","        self.dist_fc2 = nn.Linear(256, 256)\n","        self.dist_fc3 = nn.Linear(256, 256)\n","        self.dist_fc4 = nn.Linear(256, 128)\n","        self.dist_fc5 = nn.Linear(128, num_dist)\n","\n","        self.mask_fc1 = nn.Linear(512 + num_face, 256)\n","        self.mask_fc2 = nn.Linear(256, 256)\n","        self.mask_fc3 = nn.Linear(256, 256)\n","        self.mask_fc4 = nn.Linear(256, 128)\n","        self.mask_fc5 = nn.Linear(128, num_mask)\n","\n","    def _make_layer(self, out_channels, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_channels != out_channels:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","        layer = ResidualBlock(self.in_channels, out_channels, stride, downsample)\n","        self.in_channels = out_channels\n","        return layer\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","\n","        x = self.drop(x)\n","        x = self.adaptivePool(x)\n","        x_cnn_output = x.view(x.size(0), -1)\n","\n","        x_face = F.relu(self.face_fc1(x_cnn_output))\n","        x_face = F.relu(self.face_fc2(x_face))\n","        x_face = F.relu(self.face_fc3(x_face))\n","        x_face = F.relu(self.face_fc4(x_face))\n","        x_face = F.relu(self.face_fc5(x_face))\n","        x_face = F.relu(self.face_fc6(x_face))\n","        x_face = F.relu(self.face_fc7(x_face))\n","        x_face = F.relu(self.face_fc8(x_face))\n","        x_face = F.relu(self.face_fc9(x_face))\n","        x_face_output = torch.sigmoid(self.face_fc10(x_face))\n","\n","        x_dist_input = torch.cat((x_cnn_output, x_face_output), 1)\n","        x_dist = F.relu(self.dist_fc1(x_dist_input))\n","        x_dist = F.relu(self.dist_fc2(x_dist))\n","        x_dist = F.relu(self.dist_fc3(x_dist))\n","        x_dist = F.relu(self.dist_fc4(x_dist))\n","        x_dist_output = torch.sigmoid(self.dist_fc5(x_dist))\n","\n","        x_mask_input = torch.cat((x_cnn_output, x_face_output), 1)\n","        x_mask = F.relu(self.mask_fc1(x_mask_input))\n","        x_mask = F.relu(self.mask_fc2(x_mask))\n","        x_mask = F.relu(self.mask_fc3(x_mask))\n","        x_mask = F.relu(self.mask_fc4(x_mask))\n","        x_mask_output = torch.sigmoid(self.mask_fc5(x_mask))\n","\n","        return [x_face_output, x_dist_output, x_mask_output]\n","\n","model = RDNet().to(device)\n","\n","# Calculate total parameters and model size in bytes\n","param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n","buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n","total_size = param_size + buffer_size\n"],"metadata":{"id":"omJhUklEvn2u","executionInfo":{"status":"ok","timestamp":1712307452121,"user_tz":-480,"elapsed":565,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Setup training"],"metadata":{"id":"-Cqiu1XJVHNq"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"-dpgMrZvszm4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712307452121,"user_tz":-480,"elapsed":4,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"df1827f0-ee75-4a39-b864-96f4e4030841"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch size is : 128\n","Total number of batches is : 281\n","Total number of epochs is : 80\n"]}],"source":["train_cost = []\n","train_accu = []\n","\n","learning_rate = 0.000001\n","criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n","\n","training_epochs = 80\n","total_batch = len(data_train) // batch_size\n","\n","print('Batch size is : {}'.format(batch_size))\n","print('Total number of batches is : {0:2.0f}'.format(total_batch))\n","print('Total number of epochs is : {0:2.0f}'.format(training_epochs))"]},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"niJ5pyS-VL__"}},{"cell_type":"code","source":["# import torch\n","# import time\n","\n","# # Assuming `train_accu` and `train_cost` are defined earlier\n","# train_accu = []\n","# train_cost = []\n","\n","# for epoch in range(training_epochs):\n","#     avg_cost = 0\n","#     total_batches = 0\n","#     for i, (batch_X, batch_Y, sample_ids) in enumerate(data_train_loader):\n","#         total_batches += 1\n","\n","#         face_Y, dist_Y, mask_Y = [], [], []\n","#         for Y_i in batch_Y:\n","#             underline_idx = Y_i.find(\"_\")\n","#             face_Y.append(int(Y_i[underline_idx - 1]))\n","#             dist_Y.append(int(Y_i[underline_idx + 1]))\n","#             mask_Y.append(int(Y_i[underline_idx + 3]))\n","\n","#         X = batch_X.to(device)\n","#         face_Y = torch.LongTensor(face_Y).to(device)\n","#         dist_Y = torch.LongTensor(dist_Y).to(device)\n","#         mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","#         optimizer.zero_grad()\n","\n","#         output = model(X)\n","#         cost_face = criterion(output[0], face_Y)\n","#         cost_dist = criterion(output[1], dist_Y)\n","#         cost_mask = criterion(output[2], mask_Y)\n","#         cost = cost_face - 0.015 * cost_dist - 0.01 * cost_mask\n","\n","#         cost.backward()\n","#         optimizer.step()\n","\n","#         prediction = output[0].argmax(dim=1)\n","#         accuracy = (prediction == face_Y).float().mean().item()\n","\n","#         train_accu.append(accuracy)\n","#         train_cost.append(cost.item())\n","\n","#         if i % 1 == 0:\n","#             print(f\"Epoch= {epoch+1},\\t batch = {i},\\t cost = {cost.item():2.4f},\\t accuracy = {accuracy}\")\n","\n","#         avg_cost += cost.item() / total_batches\n","\n","#     print(f\"[Epoch: {epoch + 1:>4}], averaged cost = {avg_cost:.9}\")\n","#     print(total_batches)\n","\n","# print('Learning Finished!')"],"metadata":{"id":"ulsnWuD034Cv","executionInfo":{"status":"ok","timestamp":1712307452121,"user_tz":-480,"elapsed":3,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Model test - Env 1 (Meeting room)\n","\n","\n"],"metadata":{"id":"MmsGghW9VbWf"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"ZSbOSLMn5Lqe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712307989634,"user_tz":-480,"elapsed":537516,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"e3eb621c-f147-48a1-b742-1a1f7431164c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 0 averaged accuracy: 96.88 %\n","Batch 1 averaged accuracy: 95.31 %\n","Batch 2 averaged accuracy: 98.44 %\n","Batch 3 averaged accuracy: 97.66 %\n","Batch 4 averaged accuracy: 96.88 %\n","Batch 5 averaged accuracy: 96.88 %\n","Batch 6 averaged accuracy: 93.75 %\n","Batch 7 averaged accuracy: 93.75 %\n","Batch 8 averaged accuracy: 94.53 %\n","Batch 9 averaged accuracy: 96.88 %\n","Batch 10 averaged accuracy: 99.22 %\n","Batch 11 averaged accuracy: 96.88 %\n","Batch 12 averaged accuracy: 99.22 %\n","Batch 13 averaged accuracy: 96.88 %\n","Batch 14 averaged accuracy: 98.44 %\n","Batch 15 averaged accuracy: 99.22 %\n","Batch 16 averaged accuracy: 96.09 %\n","Batch 17 averaged accuracy: 92.19 %\n","Batch 18 averaged accuracy: 97.66 %\n","Batch 19 averaged accuracy: 92.97 %\n","Batch 20 averaged accuracy: 94.53 %\n","Batch 21 averaged accuracy: 92.97 %\n","Batch 22 averaged accuracy: 93.75 %\n","Batch 23 averaged accuracy: 97.66 %\n","Batch 24 averaged accuracy: 96.88 %\n","Batch 25 averaged accuracy: 90.62 %\n","Batch 26 averaged accuracy: 99.22 %\n","Batch 27 averaged accuracy: 96.88 %\n","Batch 28 averaged accuracy: 99.22 %\n","Batch 29 averaged accuracy: 96.88 %\n","Batch 30 averaged accuracy: 96.09 %\n","Batch 31 averaged accuracy: 96.88 %\n","Batch 32 averaged accuracy: 95.31 %\n","Batch 33 averaged accuracy: 96.88 %\n","Batch 34 averaged accuracy: 98.44 %\n","Batch 35 averaged accuracy: 95.31 %\n","Batch 36 averaged accuracy: 96.09 %\n","Batch 37 averaged accuracy: 96.88 %\n","Batch 38 averaged accuracy: 96.09 %\n","Batch 39 averaged accuracy: 95.31 %\n","Batch 40 averaged accuracy: 95.31 %\n","Batch 41 averaged accuracy: 97.66 %\n","Batch 42 averaged accuracy: 92.19 %\n","Batch 43 averaged accuracy: 97.66 %\n","Batch 44 averaged accuracy: 99.22 %\n","Batch 45 averaged accuracy: 93.75 %\n","Batch 46 averaged accuracy: 94.53 %\n","Batch 47 averaged accuracy: 96.88 %\n","Batch 48 averaged accuracy: 96.09 %\n","Batch 49 averaged accuracy: 96.09 %\n","Batch 50 averaged accuracy: 96.09 %\n","Batch 51 averaged accuracy: 96.88 %\n","Batch 52 averaged accuracy: 94.53 %\n","Batch 53 averaged accuracy: 96.09 %\n","Batch 54 averaged accuracy: 96.88 %\n","Batch 55 averaged accuracy: 93.75 %\n","Batch 56 averaged accuracy: 97.66 %\n","Batch 57 averaged accuracy: 92.97 %\n","Batch 58 averaged accuracy: 97.66 %\n","Batch 59 averaged accuracy: 97.66 %\n","Batch 60 averaged accuracy: 97.66 %\n","Batch 61 averaged accuracy: 96.09 %\n","Batch 62 averaged accuracy: 97.66 %\n","Batch 63 averaged accuracy: 96.09 %\n","Batch 64 averaged accuracy: 98.44 %\n","Batch 65 averaged accuracy: 95.31 %\n","Batch 66 averaged accuracy: 95.31 %\n","Batch 67 averaged accuracy: 97.66 %\n","Batch 68 averaged accuracy: 96.09 %\n","Batch 69 averaged accuracy: 96.88 %\n","Batch 70 averaged accuracy: 90.00 %\n","\n","Averaged Accuracy: 96.17 %\n","Precision - Avg: 99.05%, Median: 99.13%\n","Recall - Avg: 93.81%, Median: 93.85%\n","F1-score - Avg: 96.35%, Median: 96.43%\n"]}],"source":["import torch\n","import numpy as np\n","import time\n","\n","model_load_path = './drive/MyDrive/AcFace_AE/RD-Net/Model/model_pretrained.pth'  # The path where your model is saved\n","model.load_state_dict(torch.load(model_load_path))\n","\n","model.eval()\n","\n","acc_list = []\n","cost_list = []\n","incorrect_samples = []\n","predictions = []\n","true_labels = []\n","\n","for i, (test_X, test_Y, sample_ids) in enumerate(data_test_loader):\n","    face_Y, dist_Y, mask_Y = [], [], []\n","    for Y_i in test_Y:\n","        underline_idx = Y_i.find(\"_\")\n","        face_Y.append(int(Y_i[underline_idx-1]))\n","        dist_Y.append(int(Y_i[underline_idx+1]))\n","        mask_Y.append(int(Y_i[underline_idx+3]))\n","\n","    X = test_X.to(device)\n","    face_Y = torch.LongTensor(face_Y).to(device)\n","    dist_Y = torch.LongTensor(dist_Y).to(device)\n","    mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","    with torch.no_grad():\n","        output = model(X)\n","\n","        cost_face = criterion(output[0], face_Y)\n","        cost_dist = criterion(output[1], dist_Y)\n","        cost_mask = criterion(output[2], mask_Y)\n","        cost = cost_face - 0.015 * cost_dist - 0.01 * cost_mask\n","\n","        accuracy = (torch.max(output[0], 1)[1] == face_Y).float().mean().item()\n","\n","        acc_list.append(accuracy)\n","        cost_list.append(cost.item())\n","\n","        predictions.extend(torch.max(output[0], 1)[1].cpu().numpy())\n","        true_labels.extend(face_Y.cpu().numpy())\n","\n","        print(f'Batch {i} averaged accuracy: {accuracy*100:.2f} %')\n","\n","        incorrect_predictions = (torch.max(output[0], 1)[1] != face_Y)\n","        incorrect_indices = [i for i, x in enumerate(incorrect_predictions) if x]\n","        incorrect_samples.extend([sample_ids[idx] for idx in incorrect_indices])\n","\n","if acc_list:  # Check if acc_list is not empty\n","    print('\\nAveraged Accuracy: {:2.2f} %'.format(np.mean(acc_list) * 100))\n","else:\n","    raise Exception(\"\\nNo valid accuracy computations were performed.\")\n","\n","# if cost_list:  # Check if cost_list is not empty\n","#     print('Overall Cost: {:2.2f}'.format(np.mean(cost_list)))\n","# else:\n","#     raise Exception(\"\\nNo valid cost computations were performed.\")\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import numpy as np\n","\n","N = 10  # Specify the number of portions\n","\n","# Initialize vectors to store metrics for each portion\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","# Splitting the data into N portions and calculating metrics\n","portion_size = len(predictions) // N\n","for i in range(N):\n","    start_index = i * portion_size\n","    if i == N - 1:\n","        end_index = len(predictions)  # Ensure to include all elements in the last portion\n","    else:\n","        end_index = start_index + portion_size\n","\n","    portion_predictions = predictions[start_index:end_index]\n","    portion_true_labels = true_labels[start_index:end_index]\n","\n","    # Calculating and storing metrics for the current portion\n","    precisions.append(precision_score(portion_true_labels, portion_predictions))\n","    recalls.append(recall_score(portion_true_labels, portion_predictions))\n","    f1_scores.append(f1_score(portion_true_labels, portion_predictions))\n","\n","# Calculating averaged and median values for each metric\n","avg_precision = np.mean(precisions)\n","median_precision = np.median(precisions)\n","avg_recall = np.mean(recalls)\n","median_recall = np.median(recalls)\n","avg_f1 = np.mean(f1_scores)\n","median_f1 = np.median(f1_scores)\n","\n","# Printing the results\n","print(\"Precision - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_precision * 100, median_precision * 100))\n","print(\"Recall - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_recall * 100, median_recall * 100))\n","print(\"F1-score - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_f1 * 100, median_f1 * 100))"]},{"cell_type":"markdown","source":["## Model test - Env 2 (Laboratory)"],"metadata":{"id":"YRk3ePjaad2g"}},{"cell_type":"code","source":["class AudioFaceDataset(Dataset):\n","    def __init__(self, data_dir, split='train', transform=None, target_transform=None):\n","        self.data_dir = data_dir\n","        self.split = split\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.all_labels = self.get_all_label_df()  # Get all labels without splitting\n","        self.labels = self.split_labels()  # Split the labels according to the specified split\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels.iloc[idx]\n","        label = row[\"label\"]\n","        path = row[\"path\"]\n","        data = self.read_mat_cnn(path)\n","        if self.transform:\n","            data = self.transform(data)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        identifier = path\n","\n","        return data, label, identifier\n","\n","    @staticmethod\n","    def read_mat_cnn(file):\n","        data = loadmat(file)[\"mat_concat\"]\n","        data_tmp = np.expand_dims(data, axis=0)\n","        return data_tmp.astype(np.float32)\n","\n","    def list_all_mat_files(self):\n","        all_files = [str(x.absolute()) for x in Path(self.data_dir).glob(\"**/*.mat\")]\n","        # print(f\"Found {len(all_files)} .mat files in {self.data_dir}\")\n","        return all_files\n","\n","    def convert_path_to_label(self, path_str):\n","        label_start_idx = path_str.rfind('.mat')\n","        face_label = path_str[label_start_idx-3]\n","        mask_label = path_str[label_start_idx-2]\n","        dist_label = path_str[label_start_idx-1]\n","        return \"_\".join([face_label, dist_label, mask_label])\n","\n","    def get_all_label_df(self):\n","        label_dict = {}\n","        for file in self.list_all_mat_files():\n","            label = self.convert_path_to_label(file)\n","            label_dict[file] = label\n","\n","        label_df = pd.DataFrame.from_dict(label_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"path\", 0: \"label\"})\n","        return label_df\n","\n","    def split_labels(self):\n","        all_labels_shuffled = self.all_labels.sample(frac=1).reset_index(drop=True)\n","        if self.split == 'train':\n","            return all_labels_shuffled.sample(frac=0.8)\n","        elif self.split == 'test':\n","            return all_labels_shuffled.sample(frac=0.6)\n","        else:\n","            raise ValueError(\"Split must be 'train' or 'test'.\")\n","\n","test_dir = './drive/MyDrive/AcFace_AE/RD-Net/Dataset/Accuracy/env2_samples'\n","data_test = AudioFaceDataset(data_dir, split='test')\n","data_test_loader = DataLoader(dataset=data_test,\n","                              batch_size=batch_size,\n","                              shuffle=True,  # Typically, we don't need to shuffle the test data\n","                              num_workers=8)\n","\n","print(\"Data loader setup complete.\")"],"metadata":{"id":"MJWq30ypaigl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712308013738,"user_tz":-480,"elapsed":24129,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"3a8eea3c-fc0a-41ab-b096-776f6ab7c599"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Data loader setup complete.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import time\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import numpy as np\n","\n","model.eval()\n","\n","acc_list = []\n","cost_list = []\n","incorrect_samples = []\n","predictions = []\n","true_labels = []\n","\n","for i, (test_X, test_Y, sample_ids) in enumerate(data_test_loader):\n","    face_Y, dist_Y, mask_Y = [], [], []\n","    for Y_i in test_Y:\n","        underline_idx = Y_i.find(\"_\")\n","        face_Y.append(int(Y_i[underline_idx-1]))\n","        dist_Y.append(int(Y_i[underline_idx+1]))\n","        mask_Y.append(int(Y_i[underline_idx+3]))\n","\n","    X = test_X.to(device)\n","    face_Y = torch.LongTensor(face_Y).to(device)\n","    dist_Y = torch.LongTensor(dist_Y).to(device)\n","    mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","    with torch.no_grad():\n","        output = model(X)\n","\n","        cost_face = criterion(output[0], face_Y)\n","        cost_dist = criterion(output[1], dist_Y)\n","        cost_mask = criterion(output[2], mask_Y)\n","        cost = cost_face - 0.015 * cost_dist - 0.01 * cost_mask\n","\n","        accuracy = (torch.max(output[0], 1)[1] == face_Y).float().mean().item()\n","\n","        acc_list.append(accuracy)\n","        cost_list.append(cost.item())\n","\n","        predictions.extend(torch.max(output[0], 1)[1].cpu().numpy())\n","        true_labels.extend(face_Y.cpu().numpy())\n","\n","        print(f'Batch {i} averaged accuracy: {accuracy*100:.2f} %')\n","\n","        incorrect_predictions = (torch.max(output[0], 1)[1] != face_Y)\n","        incorrect_indices = [i for i, x in enumerate(incorrect_predictions) if x]\n","        incorrect_samples.extend([sample_ids[idx] for idx in incorrect_indices])\n","\n","if acc_list:  # Check if acc_list is not empty\n","    print('\\nAveraged Accuracy: {:2.2f} %'.format(np.mean(acc_list) * 100))\n","else:\n","    raise Exception(\"\\nNo valid accuracy computations were performed.\")\n","\n","# if cost_list:  # Check if cost_list is not empty\n","#     print('Overall Cost: {:2.2f}'.format(np.mean(cost_list)))\n","# else:\n","#     raise Exception(\"\\nNo valid cost computations were performed.\")\n","\n","N = 10  # Specify the number of portions\n","\n","# Initialize vectors to store metrics for each portion\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","# Splitting the data into N portions and calculating metrics\n","portion_size = len(predictions) // N\n","for i in range(N):\n","    start_index = i * portion_size\n","    if i == N - 1:\n","        end_index = len(predictions)  # Ensure to include all elements in the last portion\n","    else:\n","        end_index = start_index + portion_size\n","\n","    portion_predictions = predictions[start_index:end_index]\n","    portion_true_labels = true_labels[start_index:end_index]\n","\n","    # Calculating and storing metrics for the current portion\n","    precisions.append(precision_score(portion_true_labels, portion_predictions))\n","    recalls.append(recall_score(portion_true_labels, portion_predictions))\n","    f1_scores.append(f1_score(portion_true_labels, portion_predictions))\n","\n","# Calculating averaged and median values for each metric\n","avg_precision = np.mean(precisions)\n","median_precision = np.median(precisions)\n","avg_recall = np.mean(recalls)\n","median_recall = np.median(recalls)\n","avg_f1 = np.mean(f1_scores)\n","median_f1 = np.median(f1_scores)\n","\n","# Printing the results\n","print(\"Precision - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_precision * 100, median_precision * 100))\n","print(\"Recall - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_recall * 100, median_recall * 100))\n","print(\"F1-score - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_f1 * 100, median_f1 * 100))"],"metadata":{"id":"yYmWVfQPbOM3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712309579022,"user_tz":-480,"elapsed":1565288,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"f38ebd3e-434c-483d-c13b-0859d86dcb6f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 0 averaged accuracy: 96.09 %\n","Batch 1 averaged accuracy: 96.09 %\n","Batch 2 averaged accuracy: 96.88 %\n","Batch 3 averaged accuracy: 96.88 %\n","Batch 4 averaged accuracy: 96.09 %\n","Batch 5 averaged accuracy: 96.09 %\n","Batch 6 averaged accuracy: 96.88 %\n","Batch 7 averaged accuracy: 94.53 %\n","Batch 8 averaged accuracy: 94.53 %\n","Batch 9 averaged accuracy: 96.09 %\n","Batch 10 averaged accuracy: 92.97 %\n","Batch 11 averaged accuracy: 96.09 %\n","Batch 12 averaged accuracy: 96.88 %\n","Batch 13 averaged accuracy: 93.75 %\n","Batch 14 averaged accuracy: 95.31 %\n","Batch 15 averaged accuracy: 93.75 %\n","Batch 16 averaged accuracy: 97.66 %\n","Batch 17 averaged accuracy: 98.44 %\n","Batch 18 averaged accuracy: 96.09 %\n","Batch 19 averaged accuracy: 95.31 %\n","Batch 20 averaged accuracy: 97.66 %\n","Batch 21 averaged accuracy: 95.31 %\n","Batch 22 averaged accuracy: 95.31 %\n","Batch 23 averaged accuracy: 96.88 %\n","Batch 24 averaged accuracy: 96.88 %\n","Batch 25 averaged accuracy: 97.66 %\n","Batch 26 averaged accuracy: 94.53 %\n","Batch 27 averaged accuracy: 93.75 %\n","Batch 28 averaged accuracy: 99.22 %\n","Batch 29 averaged accuracy: 96.88 %\n","Batch 30 averaged accuracy: 97.66 %\n","Batch 31 averaged accuracy: 96.88 %\n","Batch 32 averaged accuracy: 94.53 %\n","Batch 33 averaged accuracy: 99.22 %\n","Batch 34 averaged accuracy: 93.75 %\n","Batch 35 averaged accuracy: 95.31 %\n","Batch 36 averaged accuracy: 96.09 %\n","Batch 37 averaged accuracy: 96.88 %\n","Batch 38 averaged accuracy: 96.88 %\n","Batch 39 averaged accuracy: 96.88 %\n","Batch 40 averaged accuracy: 96.88 %\n","Batch 41 averaged accuracy: 95.31 %\n","Batch 42 averaged accuracy: 96.88 %\n","Batch 43 averaged accuracy: 97.66 %\n","Batch 44 averaged accuracy: 94.53 %\n","Batch 45 averaged accuracy: 96.88 %\n","Batch 46 averaged accuracy: 96.09 %\n","Batch 47 averaged accuracy: 95.31 %\n","Batch 48 averaged accuracy: 94.53 %\n","Batch 49 averaged accuracy: 95.31 %\n","Batch 50 averaged accuracy: 95.31 %\n","Batch 51 averaged accuracy: 96.09 %\n","Batch 52 averaged accuracy: 93.75 %\n","Batch 53 averaged accuracy: 95.31 %\n","Batch 54 averaged accuracy: 96.88 %\n","Batch 55 averaged accuracy: 97.66 %\n","Batch 56 averaged accuracy: 96.09 %\n","Batch 57 averaged accuracy: 95.31 %\n","Batch 58 averaged accuracy: 95.31 %\n","Batch 59 averaged accuracy: 94.53 %\n","Batch 60 averaged accuracy: 96.09 %\n","Batch 61 averaged accuracy: 95.31 %\n","Batch 62 averaged accuracy: 97.66 %\n","Batch 63 averaged accuracy: 98.44 %\n","Batch 64 averaged accuracy: 96.88 %\n","Batch 65 averaged accuracy: 93.75 %\n","Batch 66 averaged accuracy: 94.53 %\n","Batch 67 averaged accuracy: 97.66 %\n","Batch 68 averaged accuracy: 96.88 %\n","Batch 69 averaged accuracy: 95.31 %\n","Batch 70 averaged accuracy: 97.66 %\n","Batch 71 averaged accuracy: 97.66 %\n","Batch 72 averaged accuracy: 95.31 %\n","Batch 73 averaged accuracy: 95.31 %\n","Batch 74 averaged accuracy: 98.44 %\n","Batch 75 averaged accuracy: 98.44 %\n","Batch 76 averaged accuracy: 96.09 %\n","Batch 77 averaged accuracy: 97.66 %\n","Batch 78 averaged accuracy: 91.41 %\n","Batch 79 averaged accuracy: 94.53 %\n","Batch 80 averaged accuracy: 94.53 %\n","Batch 81 averaged accuracy: 96.09 %\n","Batch 82 averaged accuracy: 98.44 %\n","Batch 83 averaged accuracy: 97.66 %\n","Batch 84 averaged accuracy: 98.44 %\n","Batch 85 averaged accuracy: 93.75 %\n","Batch 86 averaged accuracy: 96.88 %\n","Batch 87 averaged accuracy: 97.66 %\n","Batch 88 averaged accuracy: 97.66 %\n","Batch 89 averaged accuracy: 95.31 %\n","Batch 90 averaged accuracy: 100.00 %\n","Batch 91 averaged accuracy: 98.44 %\n","Batch 92 averaged accuracy: 96.09 %\n","Batch 93 averaged accuracy: 93.75 %\n","Batch 94 averaged accuracy: 96.09 %\n","Batch 95 averaged accuracy: 96.09 %\n","Batch 96 averaged accuracy: 99.22 %\n","Batch 97 averaged accuracy: 95.31 %\n","Batch 98 averaged accuracy: 93.75 %\n","Batch 99 averaged accuracy: 93.75 %\n","Batch 100 averaged accuracy: 99.22 %\n","Batch 101 averaged accuracy: 97.66 %\n","Batch 102 averaged accuracy: 94.53 %\n","Batch 103 averaged accuracy: 95.31 %\n","Batch 104 averaged accuracy: 96.09 %\n","Batch 105 averaged accuracy: 92.19 %\n","Batch 106 averaged accuracy: 96.09 %\n","Batch 107 averaged accuracy: 99.22 %\n","Batch 108 averaged accuracy: 95.31 %\n","Batch 109 averaged accuracy: 97.66 %\n","Batch 110 averaged accuracy: 93.75 %\n","Batch 111 averaged accuracy: 92.19 %\n","Batch 112 averaged accuracy: 96.09 %\n","Batch 113 averaged accuracy: 96.09 %\n","Batch 114 averaged accuracy: 96.09 %\n","Batch 115 averaged accuracy: 96.09 %\n","Batch 116 averaged accuracy: 95.31 %\n","Batch 117 averaged accuracy: 96.09 %\n","Batch 118 averaged accuracy: 97.66 %\n","Batch 119 averaged accuracy: 99.22 %\n","Batch 120 averaged accuracy: 96.09 %\n","Batch 121 averaged accuracy: 96.88 %\n","Batch 122 averaged accuracy: 94.53 %\n","Batch 123 averaged accuracy: 97.66 %\n","Batch 124 averaged accuracy: 96.88 %\n","Batch 125 averaged accuracy: 92.97 %\n","Batch 126 averaged accuracy: 97.66 %\n","Batch 127 averaged accuracy: 92.97 %\n","Batch 128 averaged accuracy: 92.97 %\n","Batch 129 averaged accuracy: 96.09 %\n","Batch 130 averaged accuracy: 96.88 %\n","Batch 131 averaged accuracy: 94.53 %\n","Batch 132 averaged accuracy: 98.44 %\n","Batch 133 averaged accuracy: 96.88 %\n","Batch 134 averaged accuracy: 96.88 %\n","Batch 135 averaged accuracy: 91.41 %\n","Batch 136 averaged accuracy: 90.62 %\n","Batch 137 averaged accuracy: 94.53 %\n","Batch 138 averaged accuracy: 96.09 %\n","Batch 139 averaged accuracy: 96.88 %\n","Batch 140 averaged accuracy: 94.53 %\n","Batch 141 averaged accuracy: 93.75 %\n","Batch 142 averaged accuracy: 96.88 %\n","Batch 143 averaged accuracy: 96.88 %\n","Batch 144 averaged accuracy: 95.31 %\n","Batch 145 averaged accuracy: 96.09 %\n","Batch 146 averaged accuracy: 98.44 %\n","Batch 147 averaged accuracy: 94.53 %\n","Batch 148 averaged accuracy: 97.66 %\n","Batch 149 averaged accuracy: 95.31 %\n","Batch 150 averaged accuracy: 96.09 %\n","Batch 151 averaged accuracy: 98.44 %\n","Batch 152 averaged accuracy: 95.31 %\n","Batch 153 averaged accuracy: 96.88 %\n","Batch 154 averaged accuracy: 99.22 %\n","Batch 155 averaged accuracy: 91.41 %\n","Batch 156 averaged accuracy: 96.09 %\n","Batch 157 averaged accuracy: 93.75 %\n","Batch 158 averaged accuracy: 95.31 %\n","Batch 159 averaged accuracy: 96.88 %\n","Batch 160 averaged accuracy: 96.09 %\n","Batch 161 averaged accuracy: 96.09 %\n","Batch 162 averaged accuracy: 96.09 %\n","Batch 163 averaged accuracy: 92.19 %\n","Batch 164 averaged accuracy: 96.88 %\n","Batch 165 averaged accuracy: 96.09 %\n","Batch 166 averaged accuracy: 93.75 %\n","Batch 167 averaged accuracy: 96.09 %\n","Batch 168 averaged accuracy: 97.66 %\n","Batch 169 averaged accuracy: 98.44 %\n","Batch 170 averaged accuracy: 92.97 %\n","Batch 171 averaged accuracy: 96.09 %\n","Batch 172 averaged accuracy: 92.97 %\n","Batch 173 averaged accuracy: 95.31 %\n","Batch 174 averaged accuracy: 94.53 %\n","Batch 175 averaged accuracy: 97.66 %\n","Batch 176 averaged accuracy: 96.09 %\n","Batch 177 averaged accuracy: 95.31 %\n","Batch 178 averaged accuracy: 97.66 %\n","Batch 179 averaged accuracy: 96.09 %\n","Batch 180 averaged accuracy: 95.31 %\n","Batch 181 averaged accuracy: 91.41 %\n","Batch 182 averaged accuracy: 94.53 %\n","Batch 183 averaged accuracy: 94.53 %\n","Batch 184 averaged accuracy: 96.09 %\n","Batch 185 averaged accuracy: 95.31 %\n","Batch 186 averaged accuracy: 96.09 %\n","Batch 187 averaged accuracy: 95.31 %\n","Batch 188 averaged accuracy: 94.53 %\n","Batch 189 averaged accuracy: 92.19 %\n","Batch 190 averaged accuracy: 96.09 %\n","Batch 191 averaged accuracy: 93.75 %\n","Batch 192 averaged accuracy: 98.44 %\n","Batch 193 averaged accuracy: 99.22 %\n","Batch 194 averaged accuracy: 97.66 %\n","Batch 195 averaged accuracy: 95.31 %\n","Batch 196 averaged accuracy: 92.97 %\n","Batch 197 averaged accuracy: 92.97 %\n","Batch 198 averaged accuracy: 96.88 %\n","Batch 199 averaged accuracy: 96.88 %\n","Batch 200 averaged accuracy: 96.09 %\n","Batch 201 averaged accuracy: 93.75 %\n","Batch 202 averaged accuracy: 93.75 %\n","Batch 203 averaged accuracy: 93.75 %\n","Batch 204 averaged accuracy: 96.09 %\n","Batch 205 averaged accuracy: 96.09 %\n","Batch 206 averaged accuracy: 95.31 %\n","Batch 207 averaged accuracy: 92.97 %\n","Batch 208 averaged accuracy: 96.09 %\n","Batch 209 averaged accuracy: 96.88 %\n","Batch 210 averaged accuracy: 96.67 %\n","\n","Averaged Accuracy: 95.87 %\n","Precision - Avg: 99.11%, Median: 99.14%\n","Recall - Avg: 93.12%, Median: 93.07%\n","F1-score - Avg: 96.02%, Median: 95.99%\n"]}]},{"cell_type":"markdown","source":["## Model test - Env 3 (Office)"],"metadata":{"id":"dsqM8s_fbnuY"}},{"cell_type":"code","source":["class AudioFaceDataset(Dataset):\n","    def __init__(self, data_dir, split='train', transform=None, target_transform=None):\n","        self.data_dir = data_dir\n","        self.split = split\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.all_labels = self.get_all_label_df()  # Get all labels without splitting\n","        self.labels = self.split_labels()  # Split the labels according to the specified split\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels.iloc[idx]\n","        label = row[\"label\"]\n","        path = row[\"path\"]\n","        data = self.read_mat_cnn(path)\n","        if self.transform:\n","            data = self.transform(data)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        identifier = path\n","\n","        return data, label, identifier\n","\n","    @staticmethod\n","    def read_mat_cnn(file):\n","        data = loadmat(file)[\"mat_concat\"]\n","        data_tmp = np.expand_dims(data, axis=0)\n","        return data_tmp.astype(np.float32)\n","\n","    def list_all_mat_files(self):\n","        all_files = [str(x.absolute()) for x in Path(self.data_dir).glob(\"**/*.mat\")]\n","        # print(f\"Found {len(all_files)} .mat files in {self.data_dir}\")\n","        return all_files\n","\n","    def convert_path_to_label(self, path_str):\n","        label_start_idx = path_str.rfind('.mat')\n","        face_label = path_str[label_start_idx-3]\n","        mask_label = path_str[label_start_idx-2]\n","        dist_label = path_str[label_start_idx-1]\n","        return \"_\".join([face_label, dist_label, mask_label])\n","\n","    def get_all_label_df(self):\n","        label_dict = {}\n","        for file in self.list_all_mat_files():\n","            label = self.convert_path_to_label(file)\n","            label_dict[file] = label\n","\n","        label_df = pd.DataFrame.from_dict(label_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"path\", 0: \"label\"})\n","        return label_df\n","\n","    def split_labels(self):\n","        all_labels_shuffled = self.all_labels.sample(frac=1).reset_index(drop=True)\n","        if self.split == 'train':\n","            return all_labels_shuffled.sample(frac=0.8)\n","        elif self.split == 'test':\n","            return all_labels_shuffled.sample(frac=0.6)\n","        else:\n","            raise ValueError(\"Split must be 'train' or 'test'.\")\n","\n","test_dir = './drive/MyDrive/AcFace_AE/RD-Net/Dataset/Accuracy/env3_samples'\n","data_test = AudioFaceDataset(data_dir, split='test')\n","data_test_loader = DataLoader(dataset=data_test,\n","                              batch_size=batch_size,\n","                              shuffle=True,  # Typically, we don't need to shuffle the test data\n","                              num_workers=8)\n","\n","print(\"Data loader setup complete.\")"],"metadata":{"id":"jd_a8jQDbriA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712309587741,"user_tz":-480,"elapsed":8736,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"9726d659-89e9-49fb-82e3-067f7cb7f972"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Data loader setup complete.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import time\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import numpy as np\n","\n","model.eval()\n","\n","acc_list = []\n","cost_list = []\n","incorrect_samples = []\n","predictions = []\n","true_labels = []\n","\n","for i, (test_X, test_Y, sample_ids) in enumerate(data_test_loader):\n","    face_Y, dist_Y, mask_Y = [], [], []\n","    for Y_i in test_Y:\n","        underline_idx = Y_i.find(\"_\")\n","        face_Y.append(int(Y_i[underline_idx-1]))\n","        dist_Y.append(int(Y_i[underline_idx+1]))\n","        mask_Y.append(int(Y_i[underline_idx+3]))\n","\n","    X = test_X.to(device)\n","    face_Y = torch.LongTensor(face_Y).to(device)\n","    dist_Y = torch.LongTensor(dist_Y).to(device)\n","    mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","    with torch.no_grad():\n","        output = model(X)\n","\n","        cost_face = criterion(output[0], face_Y)\n","        cost_dist = criterion(output[1], dist_Y)\n","        cost_mask = criterion(output[2], mask_Y)\n","        cost = cost_face - 0.015 * cost_dist - 0.01 * cost_mask\n","\n","        accuracy = (torch.max(output[0], 1)[1] == face_Y).float().mean().item()\n","\n","        acc_list.append(accuracy)\n","        cost_list.append(cost.item())\n","\n","        predictions.extend(torch.max(output[0], 1)[1].cpu().numpy())\n","        true_labels.extend(face_Y.cpu().numpy())\n","\n","        print(f'Batch {i} averaged accuracy: {accuracy*100:.2f} %')\n","\n","        incorrect_predictions = (torch.max(output[0], 1)[1] != face_Y)\n","        incorrect_indices = [i for i, x in enumerate(incorrect_predictions) if x]\n","        incorrect_samples.extend([sample_ids[idx] for idx in incorrect_indices])\n","\n","if acc_list:  # Check if acc_list is not empty\n","    print('\\nAveraged Accuracy: {:2.2f} %'.format(np.mean(acc_list) * 100))\n","else:\n","    raise Exception(\"\\nNo valid accuracy computations were performed.\")\n","\n","# if cost_list:  # Check if cost_list is not empty\n","#     print('Overall Cost: {:2.2f}'.format(np.mean(cost_list)))\n","# else:\n","#     raise Exception(\"\\nNo valid cost computations were performed.\")\n","\n","N = 10  # Specify the number of portions\n","\n","# Initialize vectors to store metrics for each portion\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","# Splitting the data into N portions and calculating metrics\n","portion_size = len(predictions) // N\n","for i in range(N):\n","    start_index = i * portion_size\n","    if i == N - 1:\n","        end_index = len(predictions)  # Ensure to include all elements in the last portion\n","    else:\n","        end_index = start_index + portion_size\n","\n","    portion_predictions = predictions[start_index:end_index]\n","    portion_true_labels = true_labels[start_index:end_index]\n","\n","    # Calculating and storing metrics for the current portion\n","    precisions.append(precision_score(portion_true_labels, portion_predictions))\n","    recalls.append(recall_score(portion_true_labels, portion_predictions))\n","    f1_scores.append(f1_score(portion_true_labels, portion_predictions))\n","\n","# Calculating averaged and median values for each metric\n","avg_precision = np.mean(precisions)\n","median_precision = np.median(precisions)\n","avg_recall = np.mean(recalls)\n","median_recall = np.median(recalls)\n","avg_f1 = np.mean(f1_scores)\n","median_f1 = np.median(f1_scores)\n","\n","# Printing the results\n","print(\"Precision - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_precision * 100, median_precision * 100))\n","print(\"Recall - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_recall * 100, median_recall * 100))\n","print(\"F1-score - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_f1 * 100, median_f1 * 100))"],"metadata":{"id":"YYNZy8g3cmc9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712310128726,"user_tz":-480,"elapsed":541002,"user":{"displayName":"AcFace AE","userId":"08265617518787383570"}},"outputId":"84eaf043-9bdb-4d12-89b6-2a5e6400c181"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 0 averaged accuracy: 98.44 %\n","Batch 1 averaged accuracy: 98.44 %\n","Batch 2 averaged accuracy: 96.88 %\n","Batch 3 averaged accuracy: 97.66 %\n","Batch 4 averaged accuracy: 96.88 %\n","Batch 5 averaged accuracy: 95.31 %\n","Batch 6 averaged accuracy: 95.31 %\n","Batch 7 averaged accuracy: 92.97 %\n","Batch 8 averaged accuracy: 93.75 %\n","Batch 9 averaged accuracy: 96.09 %\n","Batch 10 averaged accuracy: 96.88 %\n","Batch 11 averaged accuracy: 95.31 %\n","Batch 12 averaged accuracy: 97.66 %\n","Batch 13 averaged accuracy: 92.97 %\n","Batch 14 averaged accuracy: 93.75 %\n","Batch 15 averaged accuracy: 96.09 %\n","Batch 16 averaged accuracy: 92.19 %\n","Batch 17 averaged accuracy: 95.31 %\n","Batch 18 averaged accuracy: 95.31 %\n","Batch 19 averaged accuracy: 93.75 %\n","Batch 20 averaged accuracy: 97.66 %\n","Batch 21 averaged accuracy: 92.97 %\n","Batch 22 averaged accuracy: 95.31 %\n","Batch 23 averaged accuracy: 97.66 %\n","Batch 24 averaged accuracy: 96.88 %\n","Batch 25 averaged accuracy: 96.88 %\n","Batch 26 averaged accuracy: 96.09 %\n","Batch 27 averaged accuracy: 96.88 %\n","Batch 28 averaged accuracy: 97.66 %\n","Batch 29 averaged accuracy: 95.31 %\n","Batch 30 averaged accuracy: 94.53 %\n","Batch 31 averaged accuracy: 99.22 %\n","Batch 32 averaged accuracy: 96.88 %\n","Batch 33 averaged accuracy: 97.66 %\n","Batch 34 averaged accuracy: 98.44 %\n","Batch 35 averaged accuracy: 96.88 %\n","Batch 36 averaged accuracy: 96.09 %\n","Batch 37 averaged accuracy: 95.31 %\n","Batch 38 averaged accuracy: 97.66 %\n","Batch 39 averaged accuracy: 93.75 %\n","Batch 40 averaged accuracy: 95.31 %\n","Batch 41 averaged accuracy: 96.88 %\n","Batch 42 averaged accuracy: 95.31 %\n","Batch 43 averaged accuracy: 94.53 %\n","Batch 44 averaged accuracy: 98.44 %\n","Batch 45 averaged accuracy: 96.88 %\n","Batch 46 averaged accuracy: 94.53 %\n","Batch 47 averaged accuracy: 94.53 %\n","Batch 48 averaged accuracy: 97.66 %\n","Batch 49 averaged accuracy: 94.53 %\n","Batch 50 averaged accuracy: 95.31 %\n","Batch 51 averaged accuracy: 93.75 %\n","Batch 52 averaged accuracy: 96.09 %\n","Batch 53 averaged accuracy: 97.66 %\n","Batch 54 averaged accuracy: 96.88 %\n","Batch 55 averaged accuracy: 97.66 %\n","Batch 56 averaged accuracy: 97.66 %\n","Batch 57 averaged accuracy: 97.66 %\n","Batch 58 averaged accuracy: 97.66 %\n","Batch 59 averaged accuracy: 96.88 %\n","Batch 60 averaged accuracy: 94.53 %\n","Batch 61 averaged accuracy: 96.88 %\n","Batch 62 averaged accuracy: 92.19 %\n","Batch 63 averaged accuracy: 92.97 %\n","Batch 64 averaged accuracy: 96.88 %\n","Batch 65 averaged accuracy: 96.09 %\n","Batch 66 averaged accuracy: 98.44 %\n","Batch 67 averaged accuracy: 98.44 %\n","Batch 68 averaged accuracy: 98.44 %\n","Batch 69 averaged accuracy: 96.88 %\n","Batch 70 averaged accuracy: 95.31 %\n","Batch 71 averaged accuracy: 93.75 %\n","Batch 72 averaged accuracy: 94.53 %\n","Batch 73 averaged accuracy: 97.66 %\n","Batch 74 averaged accuracy: 96.09 %\n","Batch 75 averaged accuracy: 97.66 %\n","Batch 76 averaged accuracy: 97.66 %\n","Batch 77 averaged accuracy: 93.75 %\n","Batch 78 averaged accuracy: 99.22 %\n","Batch 79 averaged accuracy: 96.88 %\n","Batch 80 averaged accuracy: 92.97 %\n","Batch 81 averaged accuracy: 95.31 %\n","Batch 82 averaged accuracy: 98.44 %\n","Batch 83 averaged accuracy: 96.09 %\n","Batch 84 averaged accuracy: 94.53 %\n","Batch 85 averaged accuracy: 96.88 %\n","Batch 86 averaged accuracy: 96.88 %\n","Batch 87 averaged accuracy: 96.09 %\n","Batch 88 averaged accuracy: 92.97 %\n","Batch 89 averaged accuracy: 92.97 %\n","Batch 90 averaged accuracy: 98.44 %\n","Batch 91 averaged accuracy: 95.31 %\n","Batch 92 averaged accuracy: 94.53 %\n","Batch 93 averaged accuracy: 96.09 %\n","Batch 94 averaged accuracy: 94.53 %\n","Batch 95 averaged accuracy: 93.75 %\n","Batch 96 averaged accuracy: 95.31 %\n","Batch 97 averaged accuracy: 95.31 %\n","Batch 98 averaged accuracy: 95.31 %\n","Batch 99 averaged accuracy: 95.31 %\n","Batch 100 averaged accuracy: 92.97 %\n","Batch 101 averaged accuracy: 97.66 %\n","Batch 102 averaged accuracy: 96.88 %\n","Batch 103 averaged accuracy: 99.22 %\n","Batch 104 averaged accuracy: 94.53 %\n","Batch 105 averaged accuracy: 95.31 %\n","Batch 106 averaged accuracy: 97.66 %\n","Batch 107 averaged accuracy: 96.09 %\n","Batch 108 averaged accuracy: 94.53 %\n","Batch 109 averaged accuracy: 92.19 %\n","Batch 110 averaged accuracy: 96.88 %\n","Batch 111 averaged accuracy: 96.09 %\n","Batch 112 averaged accuracy: 99.22 %\n","Batch 113 averaged accuracy: 98.44 %\n","Batch 114 averaged accuracy: 91.41 %\n","Batch 115 averaged accuracy: 96.88 %\n","Batch 116 averaged accuracy: 98.44 %\n","Batch 117 averaged accuracy: 97.66 %\n","Batch 118 averaged accuracy: 96.88 %\n","Batch 119 averaged accuracy: 96.09 %\n","Batch 120 averaged accuracy: 96.88 %\n","Batch 121 averaged accuracy: 94.53 %\n","Batch 122 averaged accuracy: 94.53 %\n","Batch 123 averaged accuracy: 96.88 %\n","Batch 124 averaged accuracy: 92.19 %\n","Batch 125 averaged accuracy: 97.66 %\n","Batch 126 averaged accuracy: 96.09 %\n","Batch 127 averaged accuracy: 96.09 %\n","Batch 128 averaged accuracy: 92.19 %\n","Batch 129 averaged accuracy: 94.53 %\n","Batch 130 averaged accuracy: 96.09 %\n","Batch 131 averaged accuracy: 97.66 %\n","Batch 132 averaged accuracy: 98.44 %\n","Batch 133 averaged accuracy: 97.66 %\n","Batch 134 averaged accuracy: 96.09 %\n","Batch 135 averaged accuracy: 96.09 %\n","Batch 136 averaged accuracy: 92.97 %\n","Batch 137 averaged accuracy: 89.84 %\n","Batch 138 averaged accuracy: 96.09 %\n","Batch 139 averaged accuracy: 96.88 %\n","Batch 140 averaged accuracy: 96.09 %\n","Batch 141 averaged accuracy: 96.88 %\n","Batch 142 averaged accuracy: 96.88 %\n","Batch 143 averaged accuracy: 93.75 %\n","Batch 144 averaged accuracy: 96.88 %\n","Batch 145 averaged accuracy: 97.66 %\n","Batch 146 averaged accuracy: 96.09 %\n","Batch 147 averaged accuracy: 96.88 %\n","Batch 148 averaged accuracy: 96.88 %\n","Batch 149 averaged accuracy: 96.88 %\n","Batch 150 averaged accuracy: 90.62 %\n","Batch 151 averaged accuracy: 92.19 %\n","Batch 152 averaged accuracy: 94.53 %\n","Batch 153 averaged accuracy: 100.00 %\n","Batch 154 averaged accuracy: 93.75 %\n","Batch 155 averaged accuracy: 98.44 %\n","Batch 156 averaged accuracy: 98.44 %\n","Batch 157 averaged accuracy: 96.88 %\n","Batch 158 averaged accuracy: 96.88 %\n","Batch 159 averaged accuracy: 98.44 %\n","Batch 160 averaged accuracy: 96.88 %\n","Batch 161 averaged accuracy: 99.22 %\n","Batch 162 averaged accuracy: 98.44 %\n","Batch 163 averaged accuracy: 95.31 %\n","Batch 164 averaged accuracy: 96.09 %\n","Batch 165 averaged accuracy: 92.97 %\n","Batch 166 averaged accuracy: 96.88 %\n","Batch 167 averaged accuracy: 96.09 %\n","Batch 168 averaged accuracy: 95.31 %\n","Batch 169 averaged accuracy: 96.88 %\n","Batch 170 averaged accuracy: 97.66 %\n","Batch 171 averaged accuracy: 97.66 %\n","Batch 172 averaged accuracy: 96.09 %\n","Batch 173 averaged accuracy: 95.31 %\n","Batch 174 averaged accuracy: 97.66 %\n","Batch 175 averaged accuracy: 95.31 %\n","Batch 176 averaged accuracy: 96.88 %\n","Batch 177 averaged accuracy: 96.09 %\n","Batch 178 averaged accuracy: 96.88 %\n","Batch 179 averaged accuracy: 95.31 %\n","Batch 180 averaged accuracy: 97.66 %\n","Batch 181 averaged accuracy: 96.88 %\n","Batch 182 averaged accuracy: 96.88 %\n","Batch 183 averaged accuracy: 97.66 %\n","Batch 184 averaged accuracy: 97.66 %\n","Batch 185 averaged accuracy: 93.75 %\n","Batch 186 averaged accuracy: 94.53 %\n","Batch 187 averaged accuracy: 98.44 %\n","Batch 188 averaged accuracy: 96.88 %\n","Batch 189 averaged accuracy: 94.53 %\n","Batch 190 averaged accuracy: 96.88 %\n","Batch 191 averaged accuracy: 95.31 %\n","Batch 192 averaged accuracy: 96.09 %\n","Batch 193 averaged accuracy: 92.97 %\n","Batch 194 averaged accuracy: 94.53 %\n","Batch 195 averaged accuracy: 93.75 %\n","Batch 196 averaged accuracy: 100.00 %\n","Batch 197 averaged accuracy: 95.31 %\n","Batch 198 averaged accuracy: 95.31 %\n","Batch 199 averaged accuracy: 96.09 %\n","Batch 200 averaged accuracy: 96.09 %\n","Batch 201 averaged accuracy: 96.88 %\n","Batch 202 averaged accuracy: 96.88 %\n","Batch 203 averaged accuracy: 95.31 %\n","Batch 204 averaged accuracy: 94.53 %\n","Batch 205 averaged accuracy: 96.09 %\n","Batch 206 averaged accuracy: 98.44 %\n","Batch 207 averaged accuracy: 93.75 %\n","Batch 208 averaged accuracy: 98.44 %\n","Batch 209 averaged accuracy: 96.88 %\n","Batch 210 averaged accuracy: 96.67 %\n","\n","Averaged Accuracy: 96.05 %\n","Precision - Avg: 99.17%, Median: 99.15%\n","Recall - Avg: 93.39%, Median: 93.43%\n","F1-score - Avg: 96.19%, Median: 96.22%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
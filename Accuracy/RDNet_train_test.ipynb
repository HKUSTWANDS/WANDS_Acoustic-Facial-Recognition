{"cells":[{"cell_type":"markdown","source":["## Prepare tools"],"metadata":{"id":"GC_1XO9YRvNo"}},{"cell_type":"code","execution_count":27,"metadata":{"id":"aJYV5v6v3vp4","executionInfo":{"status":"ok","timestamp":1711420942221,"user_tz":-480,"elapsed":603,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}}},"outputs":[],"source":["import torch\n","from torch.autograd import Variable\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.nn.init\n","from scipy.io import loadmat\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711420942849,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"},"user_tz":-480},"id":"UOvELr4OAHKX","outputId":"980bfd61-3a80-4efe-f9d9-6d5dd5fba25e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of GPU:  1 <class 'torch.device'>\n","total GPU memory:  15835660288  memory reserved:  1545601024 memory allocated:  99212288\n"]}],"source":["torch.cuda.is_available()\n","\n","n_gpu = torch.cuda.device_count()\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","t = torch.cuda.get_device_properties(0).total_memory\n","r = torch.cuda.memory_reserved(0)\n","a = torch.cuda.memory_allocated(0)\n","f = r-a  # free inside reserved\n","\n","print(\"Number of GPU: \", n_gpu, type(device))\n","print(\"total GPU memory: \", t, \" memory reserved: \", r, \"memory allocated: \", a)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0rWiucdSITx","executionInfo":{"status":"ok","timestamp":1711420946142,"user_tz":-480,"elapsed":3295,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}},"outputId":"6a470545-08ed-4fc4-a490-85d44474a2e5"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"x8JAZP3DpHxL"},"source":["## Setup dataloader"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5229,"status":"ok","timestamp":1711420951369,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"},"user_tz":-480},"id":"Z_ani8bi4bhF","outputId":"c15dd343-de84-4714-f26d-eb82390e531d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data loader setup complete.\n"]}],"source":["class AudioFaceDataset(Dataset):\n","    def __init__(self, data_dir, split='train', transform=None, target_transform=None):\n","        self.data_dir = data_dir\n","        self.split = split\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.all_labels = self.get_all_label_df()  # Get all labels without splitting\n","        self.labels = self.split_labels()  # Split the labels according to the specified split\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels.iloc[idx]\n","        label = row[\"label\"]\n","        path = row[\"path\"]\n","        data = self.read_mat_cnn(path)\n","        if self.transform:\n","            data = self.transform(data)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        identifier = path\n","\n","        return data, label, identifier\n","\n","    @staticmethod\n","    def read_mat_cnn(file):\n","        data = loadmat(file)[\"mat_concat\"]\n","        data_tmp = np.expand_dims(data, axis=0)\n","        return data_tmp.astype(np.float32)\n","\n","    def list_all_mat_files(self):\n","        all_files = [str(x.absolute()) for x in Path(self.data_dir).glob(\"**/*.mat\")]\n","        # print(f\"Found {len(all_files)} .mat files in {self.data_dir}\")\n","        return all_files\n","\n","    def convert_path_to_label(self, path_str):\n","        label_start_idx = path_str.rfind('.mat')\n","        face_label = path_str[label_start_idx-3]\n","        mask_label = path_str[label_start_idx-2]\n","        dist_label = path_str[label_start_idx-1]\n","        return \"_\".join([face_label, dist_label, mask_label])\n","\n","    def get_all_label_df(self):\n","        label_dict = {}\n","        for file in self.list_all_mat_files():\n","            label = self.convert_path_to_label(file)\n","            label_dict[file] = label\n","\n","        label_df = pd.DataFrame.from_dict(label_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"path\", 0: \"label\"})\n","        return label_df\n","\n","    def split_labels(self):\n","        all_labels_shuffled = self.all_labels.sample(frac=1).reset_index(drop=True)  # Ensure reproducibility with random_state\n","        if self.split == 'train':\n","            return all_labels_shuffled.sample(frac=0.8)  # Use all data for training\n","        elif self.split == 'test':\n","            return all_labels_shuffled.sample(frac=0.2)  # Use 20% of the data for testing\n","        else:\n","            raise ValueError(\"Split must be 'train' or 'test'.\")\n","\n","# CHANGE THE FOLDER TO UNDER \"AE\"\n","data_dir = './drive/MyDrive/faceRec/Data/20220129_new'\n","# test_dir = './drive/MyDrive/AcFace AE/Performance/Data/all_samples'\n","\n","# Creating instances for training and testing\n","data_train = AudioFaceDataset(data_dir, split='train')\n","data_test = AudioFaceDataset(data_dir, split='test')\n","\n","# Setup DataLoader for training\n","batch_size = 128  # Specify your batch size\n","data_train_loader = DataLoader(dataset=data_train,\n","                               batch_size=batch_size,\n","                               shuffle=True,\n","                               num_workers=8)\n","\n","# Setup DataLoader for testing\n","data_test_loader = DataLoader(dataset=data_test,\n","                              batch_size=batch_size,\n","                              shuffle=True,  # Typically, we don't need to shuffle the test data\n","                              num_workers=8)\n","\n","print(\"Data loader setup complete.\")"]},{"cell_type":"markdown","source":["## Setup model"],"metadata":{"id":"egpfoxoKUIVG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        if self.downsample:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class RDNet(nn.Module):\n","    def __init__(self, num_face=2, num_dist=2, num_mask=2):\n","        super(RDNet, self).__init__()\n","\n","        self.in_channels = 64\n","        self.conv1 = nn.Conv2d(1, self.in_channels, kernel_size=3, stride=2, padding=1)\n","        self.bn1 = nn.BatchNorm2d(self.in_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        # Adding more depth with Residual Blocks\n","        self.layer1 = self._make_layer(128, stride=2)\n","        self.layer2 = self._make_layer(256, stride=2)\n","        self.layer3 = self._make_layer(512, stride=2)\n","        self.drop = nn.Dropout(p=0.3)\n","\n","        self.adaptivePool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # Increase model capacity in fully connected layers\n","        self.face_fc1 = nn.Linear(512, 2048)\n","        self.face_fc2 = nn.Linear(2048, 2048)\n","        self.face_fc3 = nn.Linear(2048, 1024)\n","        self.face_fc4 = nn.Linear(1024, 1024)\n","        self.face_fc5 = nn.Linear(1024, 1024)\n","        self.face_fc6 = nn.Linear(1024, 1024)\n","        self.face_fc7 = nn.Linear(1024, 1024)\n","        self.face_fc8 = nn.Linear(1024, 512)\n","        self.face_fc9 = nn.Linear(512, 512)\n","        self.face_fc10 = nn.Linear(512, num_face)\n","\n","        self.dist_fc1 = nn.Linear(512 + num_face, 256)\n","        self.dist_fc2 = nn.Linear(256, 256)\n","        self.dist_fc3 = nn.Linear(256, 256)\n","        self.dist_fc4 = nn.Linear(256, 128)\n","        self.dist_fc5 = nn.Linear(128, num_dist)\n","\n","        self.mask_fc1 = nn.Linear(512 + num_face, 256)\n","        self.mask_fc2 = nn.Linear(256, 256)\n","        self.mask_fc3 = nn.Linear(256, 256)\n","        self.mask_fc4 = nn.Linear(256, 128)\n","        self.mask_fc5 = nn.Linear(128, num_mask)\n","\n","    def _make_layer(self, out_channels, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_channels != out_channels:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","        layer = ResidualBlock(self.in_channels, out_channels, stride, downsample)\n","        self.in_channels = out_channels\n","        return layer\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","\n","        x = self.drop(x)\n","        x = self.adaptivePool(x)\n","        x_cnn_output = x.view(x.size(0), -1)\n","\n","        x_face = F.relu(self.face_fc1(x_cnn_output))\n","        x_face = F.relu(self.face_fc2(x_face))\n","        x_face = F.relu(self.face_fc3(x_face))\n","        x_face = F.relu(self.face_fc4(x_face))\n","        x_face = F.relu(self.face_fc5(x_face))\n","        x_face = F.relu(self.face_fc6(x_face))\n","        x_face = F.relu(self.face_fc7(x_face))\n","        x_face = F.relu(self.face_fc8(x_face))\n","        x_face = F.relu(self.face_fc9(x_face))\n","        x_face_output = torch.sigmoid(self.face_fc10(x_face))\n","\n","        x_dist_input = torch.cat((x_cnn_output, x_face_output), 1)\n","        x_dist = F.relu(self.dist_fc1(x_dist_input))\n","        x_dist = F.relu(self.dist_fc2(x_dist))\n","        x_dist = F.relu(self.dist_fc3(x_dist))\n","        x_dist = F.relu(self.dist_fc4(x_dist))\n","        x_dist_output = torch.sigmoid(self.dist_fc5(x_dist))\n","\n","        x_mask_input = torch.cat((x_cnn_output, x_face_output), 1)\n","        x_mask = F.relu(self.mask_fc1(x_mask_input))\n","        x_mask = F.relu(self.mask_fc2(x_mask))\n","        x_mask = F.relu(self.mask_fc3(x_mask))\n","        x_mask = F.relu(self.mask_fc4(x_mask))\n","        x_mask_output = torch.sigmoid(self.mask_fc5(x_mask))\n","\n","        return [x_face_output, x_dist_output, x_mask_output]\n","\n","model = RDNet().to(device)\n","\n","# Calculate total parameters and model size in bytes\n","param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n","buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n","total_size = param_size + buffer_size\n","\n","# Convert bytes to megabytes (MB)\n","size_in_mb = total_size / (1024 ** 2)\n","\n","print(f'Total parameters: {sum(p.numel() for p in model.parameters())}')\n","print(f'Model size: {size_in_mb:.3f} MB')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omJhUklEvn2u","executionInfo":{"status":"ok","timestamp":1711420951369,"user_tz":-480,"elapsed":14,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}},"outputId":"c42a045a-fa6f-457e-d0b0-5a958ef22ab4"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters: 17748230\n","Model size: 67.725 MB\n"]}]},{"cell_type":"markdown","source":["## Setup training"],"metadata":{"id":"-Cqiu1XJVHNq"}},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1711420951370,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"},"user_tz":-480},"id":"-dpgMrZvszm4","outputId":"46df641c-f101-4ab8-de96-b03d2a9c125e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch size is : 128\n","Total number of batches is : 281\n","Total number of epochs is : 30\n"]}],"source":["train_cost = []\n","train_accu = []\n","\n","learning_rate = 0.000001\n","criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n","\n","training_epochs = 30\n","total_batch = len(data_train) // batch_size\n","\n","print('Batch size is : {}'.format(batch_size))\n","print('Total number of batches is : {0:2.0f}'.format(total_batch))\n","print('Total number of epochs is : {0:2.0f}'.format(training_epochs))"]},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"niJ5pyS-VL__"}},{"cell_type":"code","source":["# import torch\n","# import time\n","\n","# # Assuming `train_accu` and `train_cost` are defined earlier\n","# train_accu = []\n","# train_cost = []\n","\n","# for epoch in range(training_epochs):\n","#     avg_cost = 0\n","#     total_batches = 0\n","#     for i, (batch_X, batch_Y, sample_ids) in enumerate(data_train_loader):\n","#         total_batches += 1\n","\n","#         face_Y, dist_Y, mask_Y = [], [], []\n","#         for Y_i in batch_Y:\n","#             underline_idx = Y_i.find(\"_\")\n","#             face_Y.append(int(Y_i[underline_idx - 1]))\n","#             dist_Y.append(int(Y_i[underline_idx + 1]))\n","#             mask_Y.append(int(Y_i[underline_idx + 3]))\n","\n","#         X = batch_X.to(device)\n","#         face_Y = torch.LongTensor(face_Y).to(device)\n","#         dist_Y = torch.LongTensor(dist_Y).to(device)\n","#         mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","#         optimizer.zero_grad()\n","\n","#         output = model(X)\n","#         cost_face = criterion(output[0], face_Y)\n","#         cost_dist = criterion(output[1], dist_Y)\n","#         cost_mask = criterion(output[2], mask_Y)\n","#         cost = cost_face - 0.015 * cost_dist - 0.01 * cost_mask\n","\n","#         cost.backward()\n","#         optimizer.step()\n","\n","#         prediction = output[0].argmax(dim=1)\n","#         accuracy = (prediction == face_Y).float().mean().item()\n","\n","#         train_accu.append(accuracy)\n","#         train_cost.append(cost.item())\n","\n","#         if i % 1 == 0:\n","#             print(f\"Epoch= {epoch+1},\\t batch = {i},\\t cost = {cost.item():2.4f},\\t accuracy = {accuracy}\")\n","\n","#         avg_cost += cost.item() / total_batches\n","\n","#     print(f\"[Epoch: {epoch + 1:>4}], averaged cost = {avg_cost:.9}\")\n","#     print(total_batches)\n","\n","# print('Learning Finished!')"],"metadata":{"id":"ulsnWuD034Cv","executionInfo":{"status":"ok","timestamp":1711420951370,"user_tz":-480,"elapsed":11,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## Save model"],"metadata":{"id":"Fx6gaPbJVXYG"}},{"cell_type":"code","source":["# # Save the model after training\n","# model_save_path = './drive/MyDrive/faceRec/code/model_trained_50epoch.pth'  # Specify your path here\n","# torch.save(model.state_dict(), model_save_path)\n","# print(f'Model saved to {model_save_path}')"],"metadata":{"id":"l5PqMTklvSA0","executionInfo":{"status":"ok","timestamp":1711420951370,"user_tz":-480,"elapsed":11,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["## Model test\n","\n","\n"],"metadata":{"id":"MmsGghW9VbWf"}},{"cell_type":"code","execution_count":35,"metadata":{"id":"ZSbOSLMn5Lqe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711421441963,"user_tz":-480,"elapsed":490604,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}},"outputId":"663a71e7-4163-4fe8-d917-b8a372dac0c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 0 averaged accuracy: 98.44 %\n","Batch 1 averaged accuracy: 96.88 %\n","Batch 2 averaged accuracy: 97.66 %\n","Batch 3 averaged accuracy: 97.66 %\n","Batch 4 averaged accuracy: 96.09 %\n","Batch 5 averaged accuracy: 95.31 %\n","Batch 6 averaged accuracy: 98.44 %\n","Batch 7 averaged accuracy: 96.88 %\n","Batch 8 averaged accuracy: 97.66 %\n","Batch 9 averaged accuracy: 93.75 %\n","Batch 10 averaged accuracy: 97.66 %\n","Batch 11 averaged accuracy: 97.66 %\n","Batch 12 averaged accuracy: 96.88 %\n","Batch 13 averaged accuracy: 95.31 %\n","Batch 14 averaged accuracy: 96.09 %\n","Batch 15 averaged accuracy: 97.66 %\n","Batch 16 averaged accuracy: 94.53 %\n","Batch 17 averaged accuracy: 96.09 %\n","Batch 18 averaged accuracy: 96.09 %\n","Batch 19 averaged accuracy: 96.88 %\n","Batch 20 averaged accuracy: 93.75 %\n","Batch 21 averaged accuracy: 96.09 %\n","Batch 22 averaged accuracy: 96.88 %\n","Batch 23 averaged accuracy: 97.66 %\n","Batch 24 averaged accuracy: 92.97 %\n","Batch 25 averaged accuracy: 94.53 %\n","Batch 26 averaged accuracy: 96.88 %\n","Batch 27 averaged accuracy: 95.31 %\n","Batch 28 averaged accuracy: 94.53 %\n","Batch 29 averaged accuracy: 96.09 %\n","Batch 30 averaged accuracy: 96.88 %\n","Batch 31 averaged accuracy: 96.88 %\n","Batch 32 averaged accuracy: 96.09 %\n","Batch 33 averaged accuracy: 96.88 %\n","Batch 34 averaged accuracy: 97.66 %\n","Batch 35 averaged accuracy: 96.88 %\n","Batch 36 averaged accuracy: 95.31 %\n","Batch 37 averaged accuracy: 98.44 %\n","Batch 38 averaged accuracy: 96.09 %\n","Batch 39 averaged accuracy: 95.31 %\n","Batch 40 averaged accuracy: 96.09 %\n","Batch 41 averaged accuracy: 96.09 %\n","Batch 42 averaged accuracy: 95.31 %\n","Batch 43 averaged accuracy: 95.31 %\n","Batch 44 averaged accuracy: 94.53 %\n","Batch 45 averaged accuracy: 99.22 %\n","Batch 46 averaged accuracy: 96.88 %\n","Batch 47 averaged accuracy: 97.66 %\n","Batch 48 averaged accuracy: 94.53 %\n","Batch 49 averaged accuracy: 96.88 %\n","Batch 50 averaged accuracy: 96.88 %\n","Batch 51 averaged accuracy: 94.53 %\n","Batch 52 averaged accuracy: 96.09 %\n","Batch 53 averaged accuracy: 96.09 %\n","Batch 54 averaged accuracy: 96.09 %\n","Batch 55 averaged accuracy: 94.53 %\n","Batch 56 averaged accuracy: 93.75 %\n","Batch 57 averaged accuracy: 97.66 %\n","Batch 58 averaged accuracy: 97.66 %\n","Batch 59 averaged accuracy: 96.88 %\n","Batch 60 averaged accuracy: 96.09 %\n","Batch 61 averaged accuracy: 96.09 %\n","Batch 62 averaged accuracy: 95.31 %\n","Batch 63 averaged accuracy: 95.31 %\n","Batch 64 averaged accuracy: 98.44 %\n","Batch 65 averaged accuracy: 98.44 %\n","Batch 66 averaged accuracy: 97.66 %\n","Batch 67 averaged accuracy: 96.88 %\n","Batch 68 averaged accuracy: 95.31 %\n","Batch 69 averaged accuracy: 92.19 %\n","Batch 70 averaged accuracy: 97.50 %\n","\n","Overall Accuracy: 96.29 %\n","Overall Cost: 0.34\n"]}],"source":["import torch\n","import numpy as np\n","import time\n","\n","model_load_path = './drive/MyDrive/AcFace AE/Performance/Model/model_trained_2.pth'  # The path where your model is saved\n","model.load_state_dict(torch.load(model_load_path))\n","\n","model.eval()\n","\n","acc_list = []\n","cost_list = []\n","incorrect_samples = []\n","predictions = []\n","true_labels = []\n","\n","for i, (test_X, test_Y, sample_ids) in enumerate(data_test_loader):\n","    face_Y, dist_Y, mask_Y = [], [], []\n","    for Y_i in test_Y:\n","        underline_idx = Y_i.find(\"_\")\n","        face_Y.append(int(Y_i[underline_idx-1]))\n","        dist_Y.append(int(Y_i[underline_idx+1]))\n","        mask_Y.append(int(Y_i[underline_idx+3]))\n","\n","    X = test_X.to(device)\n","    face_Y = torch.LongTensor(face_Y).to(device)\n","    dist_Y = torch.LongTensor(dist_Y).to(device)\n","    mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","    with torch.no_grad():\n","        output = model(X)\n","\n","        cost_face = criterion(output[0], face_Y)\n","        cost_dist = criterion(output[1], dist_Y)\n","        cost_mask = criterion(output[2], mask_Y)\n","        cost = cost_face - 0.015 * cost_dist - 0.01 * cost_mask\n","\n","        accuracy = (torch.max(output[0], 1)[1] == face_Y).float().mean().item()\n","\n","        acc_list.append(accuracy)\n","        cost_list.append(cost.item())\n","\n","        predictions.extend(torch.max(output[0], 1)[1].cpu().numpy())\n","        true_labels.extend(face_Y.cpu().numpy())\n","\n","        print(f'Batch {i} averaged accuracy: {accuracy*100:.2f} %')\n","\n","        incorrect_predictions = (torch.max(output[0], 1)[1] != face_Y)\n","        incorrect_indices = [i for i, x in enumerate(incorrect_predictions) if x]\n","        incorrect_samples.extend([sample_ids[idx] for idx in incorrect_indices])\n","\n","if acc_list:  # Check if acc_list is not empty\n","    print('\\nOverall Accuracy: {:2.2f} %'.format(np.mean(acc_list) * 100))\n","else:\n","    raise Exception(\"\\nNo valid accuracy computations were performed.\")\n","\n","if cost_list:  # Check if cost_list is not empty\n","    print('Overall Cost: {:2.2f}'.format(np.mean(cost_list)))\n","else:\n","    raise Exception(\"\\nNo valid cost computations were performed.\")\n"]},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","import numpy as np\n","\n","N = 10  # Specify the number of portions\n","\n","# Initialize vectors to store metrics for each portion\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","# Splitting the data into N portions and calculating metrics\n","portion_size = len(predictions) // N\n","for i in range(N):\n","    start_index = i * portion_size\n","    if i == N - 1:\n","        end_index = len(predictions)  # Ensure to include all elements in the last portion\n","    else:\n","        end_index = start_index + portion_size\n","\n","    portion_predictions = predictions[start_index:end_index]\n","    portion_true_labels = true_labels[start_index:end_index]\n","\n","    # Calculating and storing metrics for the current portion\n","    precisions.append(precision_score(portion_true_labels, portion_predictions))\n","    recalls.append(recall_score(portion_true_labels, portion_predictions))\n","    f1_scores.append(f1_score(portion_true_labels, portion_predictions))\n","\n","# Calculating averaged and median values for each metric\n","avg_precision = np.mean(precisions)\n","median_precision = np.median(precisions)\n","avg_recall = np.mean(recalls)\n","median_recall = np.median(recalls)\n","avg_f1 = np.mean(f1_scores)\n","median_f1 = np.median(f1_scores)\n","\n","# Printing the results\n","print(\"Precision - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_precision * 100, median_precision * 100))\n","print(\"Recall - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_recall * 100, median_recall * 100))\n","print(\"F1-score - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_f1 * 100, median_f1 * 100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1Jiom2LZHX_","executionInfo":{"status":"ok","timestamp":1711421441964,"user_tz":-480,"elapsed":32,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}},"outputId":"d4dfa3d7-2c8e-44f4-a8be-5853a25128c4"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision - Avg: 93.58%, Median: 93.60%\n","Recall - Avg: 100.00%, Median: 100.00%\n","F1-score - Avg: 96.68%, Median: 96.70%\n"]}]},{"cell_type":"markdown","source":["## Model test - Env 2"],"metadata":{"id":"YRk3ePjaad2g"}},{"cell_type":"code","source":["class AudioFaceDataset(Dataset):\n","    def __init__(self, data_dir, split='train', transform=None, target_transform=None):\n","        self.data_dir = data_dir\n","        self.split = split\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.all_labels = self.get_all_label_df()  # Get all labels without splitting\n","        self.labels = self.split_labels()  # Split the labels according to the specified split\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels.iloc[idx]\n","        label = row[\"label\"]\n","        path = row[\"path\"]\n","        data = self.read_mat_cnn(path)\n","        if self.transform:\n","            data = self.transform(data)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        identifier = path\n","\n","        return data, label, identifier\n","\n","    @staticmethod\n","    def read_mat_cnn(file):\n","        data = loadmat(file)[\"mat_concat\"]\n","        data_tmp = np.expand_dims(data, axis=0)\n","        return data_tmp.astype(np.float32)\n","\n","    def list_all_mat_files(self):\n","        all_files = [str(x.absolute()) for x in Path(self.data_dir).glob(\"**/*.mat\")]\n","        # print(f\"Found {len(all_files)} .mat files in {self.data_dir}\")\n","        return all_files\n","\n","    def convert_path_to_label(self, path_str):\n","        label_start_idx = path_str.rfind('.mat')\n","        face_label = path_str[label_start_idx-3]\n","        mask_label = path_str[label_start_idx-2]\n","        dist_label = path_str[label_start_idx-1]\n","        return \"_\".join([face_label, dist_label, mask_label])\n","\n","    def get_all_label_df(self):\n","        label_dict = {}\n","        for file in self.list_all_mat_files():\n","            label = self.convert_path_to_label(file)\n","            label_dict[file] = label\n","\n","        label_df = pd.DataFrame.from_dict(label_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"path\", 0: \"label\"})\n","        return label_df\n","\n","    def split_labels(self):\n","        all_labels_shuffled = self.all_labels.sample(frac=1).reset_index(drop=True)\n","        if self.split == 'train':\n","            return all_labels_shuffled.sample(frac=0.8)\n","        elif self.split == 'test':\n","            return all_labels_shuffled.sample(frac=0.6)\n","        else:\n","            raise ValueError(\"Split must be 'train' or 'test'.\")\n","\n","test_dir = './drive/MyDrive/AcFace AE/Performance/Data/env2_samples'\n","data_test = AudioFaceDataset(data_dir, split='test')\n","data_test_loader = DataLoader(dataset=data_test,\n","                              batch_size=batch_size,\n","                              shuffle=True,  # Typically, we don't need to shuffle the test data\n","                              num_workers=8)\n","\n","print(\"Data loader setup complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJWq30ypaigl","executionInfo":{"status":"ok","timestamp":1711421444196,"user_tz":-480,"elapsed":2260,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}},"outputId":"144a1573-9b5d-4a14-8bcd-69d4059157a7"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Data loader setup complete.\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import time\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import numpy as np\n","\n","model_load_path = './drive/MyDrive/AcFace AE/Performance/Model/model_trained_2.pth'  # The path where your model is saved\n","model.load_state_dict(torch.load(model_load_path))\n","\n","model.eval()\n","\n","acc_list = []\n","cost_list = []\n","incorrect_samples = []\n","predictions = []\n","true_labels = []\n","\n","for i, (test_X, test_Y, sample_ids) in enumerate(data_test_loader):\n","    face_Y, dist_Y, mask_Y = [], [], []\n","    for Y_i in test_Y:\n","        underline_idx = Y_i.find(\"_\")\n","        face_Y.append(int(Y_i[underline_idx-1]))\n","        dist_Y.append(int(Y_i[underline_idx+1]))\n","        mask_Y.append(int(Y_i[underline_idx+3]))\n","\n","    X = test_X.to(device)\n","    face_Y = torch.LongTensor(face_Y).to(device)\n","    dist_Y = torch.LongTensor(dist_Y).to(device)\n","    mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","    with torch.no_grad():\n","        output = model(X)\n","\n","        cost_face = criterion(output[0], face_Y)\n","        cost_dist = criterion(output[1], dist_Y)\n","        cost_mask = criterion(output[2], mask_Y)\n","        cost = cost_face - 0.015 * cost_dist - 0.01 * cost_mask\n","\n","        accuracy = (torch.max(output[0], 1)[1] == face_Y).float().mean().item()\n","\n","        acc_list.append(accuracy)\n","        cost_list.append(cost.item())\n","\n","        predictions.extend(torch.max(output[0], 1)[1].cpu().numpy())\n","        true_labels.extend(face_Y.cpu().numpy())\n","\n","        print(f'Batch {i} averaged accuracy: {accuracy*100:.2f} %')\n","\n","        incorrect_predictions = (torch.max(output[0], 1)[1] != face_Y)\n","        incorrect_indices = [i for i, x in enumerate(incorrect_predictions) if x]\n","        incorrect_samples.extend([sample_ids[idx] for idx in incorrect_indices])\n","\n","if acc_list:  # Check if acc_list is not empty\n","    print('\\nOverall Accuracy: {:2.2f} %'.format(np.mean(acc_list) * 100))\n","else:\n","    raise Exception(\"\\nNo valid accuracy computations were performed.\")\n","\n","if cost_list:  # Check if cost_list is not empty\n","    print('Overall Cost: {:2.2f}'.format(np.mean(cost_list)))\n","else:\n","    raise Exception(\"\\nNo valid cost computations were performed.\")\n","\n","N = 10  # Specify the number of portions\n","\n","# Initialize vectors to store metrics for each portion\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","# Splitting the data into N portions and calculating metrics\n","portion_size = len(predictions) // N\n","for i in range(N):\n","    start_index = i * portion_size\n","    if i == N - 1:\n","        end_index = len(predictions)  # Ensure to include all elements in the last portion\n","    else:\n","        end_index = start_index + portion_size\n","\n","    portion_predictions = predictions[start_index:end_index]\n","    portion_true_labels = true_labels[start_index:end_index]\n","\n","    # Calculating and storing metrics for the current portion\n","    precisions.append(precision_score(portion_true_labels, portion_predictions))\n","    recalls.append(recall_score(portion_true_labels, portion_predictions))\n","    f1_scores.append(f1_score(portion_true_labels, portion_predictions))\n","\n","# Calculating averaged and median values for each metric\n","avg_precision = np.mean(precisions)\n","median_precision = np.median(precisions)\n","avg_recall = np.mean(recalls)\n","median_recall = np.median(recalls)\n","avg_f1 = np.mean(f1_scores)\n","median_f1 = np.median(f1_scores)\n","\n","# Printing the results\n","print(\"Precision - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_precision * 100, median_precision * 100))\n","print(\"Recall - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_recall * 100, median_recall * 100))\n","print(\"F1-score - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_f1 * 100, median_f1 * 100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yYmWVfQPbOM3","executionInfo":{"status":"ok","timestamp":1711421517391,"user_tz":-480,"elapsed":73198,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}},"outputId":"75b98c1a-f176-47bb-b507-e77c69d846f2"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 0 averaged accuracy: 98.44 %\n","Batch 1 averaged accuracy: 97.66 %\n","Batch 2 averaged accuracy: 93.75 %\n","Batch 3 averaged accuracy: 97.66 %\n","Batch 4 averaged accuracy: 98.44 %\n","Batch 5 averaged accuracy: 95.31 %\n","Batch 6 averaged accuracy: 96.09 %\n","Batch 7 averaged accuracy: 97.66 %\n","Batch 8 averaged accuracy: 94.53 %\n","Batch 9 averaged accuracy: 92.97 %\n","Batch 10 averaged accuracy: 96.88 %\n","Batch 11 averaged accuracy: 94.53 %\n","Batch 12 averaged accuracy: 100.00 %\n","Batch 13 averaged accuracy: 97.66 %\n","Batch 14 averaged accuracy: 98.44 %\n","Batch 15 averaged accuracy: 96.09 %\n","Batch 16 averaged accuracy: 96.09 %\n","Batch 17 averaged accuracy: 94.53 %\n","Batch 18 averaged accuracy: 92.97 %\n","Batch 19 averaged accuracy: 95.31 %\n","Batch 20 averaged accuracy: 95.31 %\n","Batch 21 averaged accuracy: 97.66 %\n","Batch 22 averaged accuracy: 97.66 %\n","Batch 23 averaged accuracy: 97.66 %\n","Batch 24 averaged accuracy: 96.88 %\n","Batch 25 averaged accuracy: 96.09 %\n","Batch 26 averaged accuracy: 96.88 %\n","Batch 27 averaged accuracy: 97.66 %\n","Batch 28 averaged accuracy: 96.88 %\n","Batch 29 averaged accuracy: 97.66 %\n","Batch 30 averaged accuracy: 97.66 %\n","Batch 31 averaged accuracy: 94.53 %\n","Batch 32 averaged accuracy: 94.53 %\n","Batch 33 averaged accuracy: 97.66 %\n","Batch 34 averaged accuracy: 97.66 %\n","Batch 35 averaged accuracy: 96.09 %\n","Batch 36 averaged accuracy: 96.88 %\n","Batch 37 averaged accuracy: 90.62 %\n","Batch 38 averaged accuracy: 96.88 %\n","Batch 39 averaged accuracy: 97.66 %\n","Batch 40 averaged accuracy: 98.44 %\n","Batch 41 averaged accuracy: 92.97 %\n","Batch 42 averaged accuracy: 98.44 %\n","Batch 43 averaged accuracy: 99.22 %\n","Batch 44 averaged accuracy: 96.88 %\n","Batch 45 averaged accuracy: 97.66 %\n","Batch 46 averaged accuracy: 95.31 %\n","Batch 47 averaged accuracy: 95.31 %\n","Batch 48 averaged accuracy: 97.66 %\n","Batch 49 averaged accuracy: 92.97 %\n","Batch 50 averaged accuracy: 92.19 %\n","Batch 51 averaged accuracy: 95.31 %\n","Batch 52 averaged accuracy: 98.44 %\n","Batch 53 averaged accuracy: 96.09 %\n","Batch 54 averaged accuracy: 96.88 %\n","Batch 55 averaged accuracy: 97.66 %\n","Batch 56 averaged accuracy: 99.22 %\n","Batch 57 averaged accuracy: 96.09 %\n","Batch 58 averaged accuracy: 99.22 %\n","Batch 59 averaged accuracy: 98.44 %\n","Batch 60 averaged accuracy: 94.53 %\n","Batch 61 averaged accuracy: 97.66 %\n","Batch 62 averaged accuracy: 96.88 %\n","Batch 63 averaged accuracy: 96.88 %\n","Batch 64 averaged accuracy: 93.75 %\n","Batch 65 averaged accuracy: 93.75 %\n","Batch 66 averaged accuracy: 96.88 %\n","Batch 67 averaged accuracy: 93.75 %\n","Batch 68 averaged accuracy: 96.88 %\n","Batch 69 averaged accuracy: 96.09 %\n","Batch 70 averaged accuracy: 96.88 %\n","Batch 71 averaged accuracy: 96.09 %\n","Batch 72 averaged accuracy: 92.97 %\n","Batch 73 averaged accuracy: 94.53 %\n","Batch 74 averaged accuracy: 95.31 %\n","Batch 75 averaged accuracy: 96.88 %\n","Batch 76 averaged accuracy: 93.75 %\n","Batch 77 averaged accuracy: 91.41 %\n","Batch 78 averaged accuracy: 97.66 %\n","Batch 79 averaged accuracy: 96.88 %\n","Batch 80 averaged accuracy: 97.66 %\n","Batch 81 averaged accuracy: 96.09 %\n","Batch 82 averaged accuracy: 97.66 %\n","Batch 83 averaged accuracy: 96.09 %\n","Batch 84 averaged accuracy: 97.66 %\n","Batch 85 averaged accuracy: 95.31 %\n","Batch 86 averaged accuracy: 98.44 %\n","Batch 87 averaged accuracy: 94.53 %\n","Batch 88 averaged accuracy: 98.44 %\n","Batch 89 averaged accuracy: 91.41 %\n","Batch 90 averaged accuracy: 93.75 %\n","Batch 91 averaged accuracy: 95.31 %\n","Batch 92 averaged accuracy: 95.31 %\n","Batch 93 averaged accuracy: 94.53 %\n","Batch 94 averaged accuracy: 96.88 %\n","Batch 95 averaged accuracy: 94.53 %\n","Batch 96 averaged accuracy: 95.31 %\n","Batch 97 averaged accuracy: 97.66 %\n","Batch 98 averaged accuracy: 98.44 %\n","Batch 99 averaged accuracy: 94.53 %\n","Batch 100 averaged accuracy: 98.44 %\n","Batch 101 averaged accuracy: 94.53 %\n","Batch 102 averaged accuracy: 96.09 %\n","Batch 103 averaged accuracy: 94.53 %\n","Batch 104 averaged accuracy: 96.09 %\n","Batch 105 averaged accuracy: 92.97 %\n","Batch 106 averaged accuracy: 98.44 %\n","Batch 107 averaged accuracy: 96.88 %\n","Batch 108 averaged accuracy: 98.44 %\n","Batch 109 averaged accuracy: 96.09 %\n","Batch 110 averaged accuracy: 94.53 %\n","Batch 111 averaged accuracy: 98.44 %\n","Batch 112 averaged accuracy: 94.53 %\n","Batch 113 averaged accuracy: 92.97 %\n","Batch 114 averaged accuracy: 96.09 %\n","Batch 115 averaged accuracy: 96.88 %\n","Batch 116 averaged accuracy: 96.09 %\n","Batch 117 averaged accuracy: 95.31 %\n","Batch 118 averaged accuracy: 96.09 %\n","Batch 119 averaged accuracy: 96.88 %\n","Batch 120 averaged accuracy: 96.88 %\n","Batch 121 averaged accuracy: 92.97 %\n","Batch 122 averaged accuracy: 99.22 %\n","Batch 123 averaged accuracy: 98.44 %\n","Batch 124 averaged accuracy: 96.88 %\n","Batch 125 averaged accuracy: 94.53 %\n","Batch 126 averaged accuracy: 92.97 %\n","Batch 127 averaged accuracy: 95.31 %\n","Batch 128 averaged accuracy: 96.09 %\n","Batch 129 averaged accuracy: 96.88 %\n","Batch 130 averaged accuracy: 95.31 %\n","Batch 131 averaged accuracy: 94.53 %\n","Batch 132 averaged accuracy: 96.88 %\n","Batch 133 averaged accuracy: 96.88 %\n","Batch 134 averaged accuracy: 99.22 %\n","Batch 135 averaged accuracy: 94.53 %\n","Batch 136 averaged accuracy: 98.44 %\n","Batch 137 averaged accuracy: 96.09 %\n","Batch 138 averaged accuracy: 96.09 %\n","Batch 139 averaged accuracy: 98.44 %\n","Batch 140 averaged accuracy: 96.09 %\n","Batch 141 averaged accuracy: 97.66 %\n","Batch 142 averaged accuracy: 96.09 %\n","Batch 143 averaged accuracy: 96.09 %\n","Batch 144 averaged accuracy: 96.88 %\n","Batch 145 averaged accuracy: 96.09 %\n","Batch 146 averaged accuracy: 96.09 %\n","Batch 147 averaged accuracy: 95.31 %\n","Batch 148 averaged accuracy: 94.53 %\n","Batch 149 averaged accuracy: 92.19 %\n","Batch 150 averaged accuracy: 96.09 %\n","Batch 151 averaged accuracy: 96.09 %\n","Batch 152 averaged accuracy: 92.19 %\n","Batch 153 averaged accuracy: 95.31 %\n","Batch 154 averaged accuracy: 98.44 %\n","Batch 155 averaged accuracy: 93.75 %\n","Batch 156 averaged accuracy: 93.75 %\n","Batch 157 averaged accuracy: 96.88 %\n","Batch 158 averaged accuracy: 97.66 %\n","Batch 159 averaged accuracy: 97.66 %\n","Batch 160 averaged accuracy: 97.66 %\n","Batch 161 averaged accuracy: 98.44 %\n","Batch 162 averaged accuracy: 94.53 %\n","Batch 163 averaged accuracy: 96.09 %\n","Batch 164 averaged accuracy: 96.09 %\n","Batch 165 averaged accuracy: 94.53 %\n","Batch 166 averaged accuracy: 96.09 %\n","Batch 167 averaged accuracy: 99.22 %\n","Batch 168 averaged accuracy: 98.44 %\n","Batch 169 averaged accuracy: 97.66 %\n","Batch 170 averaged accuracy: 96.88 %\n","Batch 171 averaged accuracy: 92.97 %\n","Batch 172 averaged accuracy: 97.66 %\n","Batch 173 averaged accuracy: 97.66 %\n","Batch 174 averaged accuracy: 93.75 %\n","Batch 175 averaged accuracy: 96.09 %\n","Batch 176 averaged accuracy: 96.09 %\n","Batch 177 averaged accuracy: 96.88 %\n","Batch 178 averaged accuracy: 98.44 %\n","Batch 179 averaged accuracy: 97.66 %\n","Batch 180 averaged accuracy: 92.19 %\n","Batch 181 averaged accuracy: 98.44 %\n","Batch 182 averaged accuracy: 98.44 %\n","Batch 183 averaged accuracy: 96.88 %\n","Batch 184 averaged accuracy: 96.88 %\n","Batch 185 averaged accuracy: 98.44 %\n","Batch 186 averaged accuracy: 96.09 %\n","Batch 187 averaged accuracy: 95.31 %\n","Batch 188 averaged accuracy: 96.09 %\n","Batch 189 averaged accuracy: 96.09 %\n","Batch 190 averaged accuracy: 95.31 %\n","Batch 191 averaged accuracy: 95.31 %\n","Batch 192 averaged accuracy: 96.09 %\n","Batch 193 averaged accuracy: 94.53 %\n","Batch 194 averaged accuracy: 95.31 %\n","Batch 195 averaged accuracy: 95.31 %\n","Batch 196 averaged accuracy: 92.97 %\n","Batch 197 averaged accuracy: 95.31 %\n","Batch 198 averaged accuracy: 96.88 %\n","Batch 199 averaged accuracy: 96.88 %\n","Batch 200 averaged accuracy: 95.31 %\n","Batch 201 averaged accuracy: 96.88 %\n","Batch 202 averaged accuracy: 100.00 %\n","Batch 203 averaged accuracy: 96.88 %\n","Batch 204 averaged accuracy: 96.88 %\n","Batch 205 averaged accuracy: 94.53 %\n","Batch 206 averaged accuracy: 98.44 %\n","Batch 207 averaged accuracy: 94.53 %\n","Batch 208 averaged accuracy: 95.31 %\n","Batch 209 averaged accuracy: 95.31 %\n","Batch 210 averaged accuracy: 95.83 %\n","\n","Overall Accuracy: 96.17 %\n","Overall Cost: 0.34\n","Precision - Avg: 93.27%, Median: 93.29%\n","Recall - Avg: 100.00%, Median: 100.00%\n","F1-score - Avg: 96.52%, Median: 96.53%\n"]}]},{"cell_type":"markdown","source":["## Model test - Env 3"],"metadata":{"id":"dsqM8s_fbnuY"}},{"cell_type":"code","source":["class AudioFaceDataset(Dataset):\n","    def __init__(self, data_dir, split='train', transform=None, target_transform=None):\n","        self.data_dir = data_dir\n","        self.split = split\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.all_labels = self.get_all_label_df()  # Get all labels without splitting\n","        self.labels = self.split_labels()  # Split the labels according to the specified split\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels.iloc[idx]\n","        label = row[\"label\"]\n","        path = row[\"path\"]\n","        data = self.read_mat_cnn(path)\n","        if self.transform:\n","            data = self.transform(data)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        identifier = path\n","\n","        return data, label, identifier\n","\n","    @staticmethod\n","    def read_mat_cnn(file):\n","        data = loadmat(file)[\"mat_concat\"]\n","        data_tmp = np.expand_dims(data, axis=0)\n","        return data_tmp.astype(np.float32)\n","\n","    def list_all_mat_files(self):\n","        all_files = [str(x.absolute()) for x in Path(self.data_dir).glob(\"**/*.mat\")]\n","        # print(f\"Found {len(all_files)} .mat files in {self.data_dir}\")\n","        return all_files\n","\n","    def convert_path_to_label(self, path_str):\n","        label_start_idx = path_str.rfind('.mat')\n","        face_label = path_str[label_start_idx-3]\n","        mask_label = path_str[label_start_idx-2]\n","        dist_label = path_str[label_start_idx-1]\n","        return \"_\".join([face_label, dist_label, mask_label])\n","\n","    def get_all_label_df(self):\n","        label_dict = {}\n","        for file in self.list_all_mat_files():\n","            label = self.convert_path_to_label(file)\n","            label_dict[file] = label\n","\n","        label_df = pd.DataFrame.from_dict(label_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"path\", 0: \"label\"})\n","        return label_df\n","\n","    def split_labels(self):\n","        all_labels_shuffled = self.all_labels.sample(frac=1).reset_index(drop=True)\n","        if self.split == 'train':\n","            return all_labels_shuffled.sample(frac=0.8)\n","        elif self.split == 'test':\n","            return all_labels_shuffled.sample(frac=0.6)\n","        else:\n","            raise ValueError(\"Split must be 'train' or 'test'.\")\n","\n","test_dir = './drive/MyDrive/AcFace AE/Performance/Data/env3_samples'\n","data_test = AudioFaceDataset(data_dir, split='test')\n","data_test_loader = DataLoader(dataset=data_test,\n","                              batch_size=batch_size,\n","                              shuffle=True,  # Typically, we don't need to shuffle the test data\n","                              num_workers=8)\n","\n","print(\"Data loader setup complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jd_a8jQDbriA","executionInfo":{"status":"ok","timestamp":1711421526882,"user_tz":-480,"elapsed":9501,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}},"outputId":"bc6c23da-0c2f-4ce9-9168-51407459797b"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Data loader setup complete.\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import time\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import numpy as np\n","\n","model_load_path = './drive/MyDrive/AcFace AE/Performance/Model/model_trained_2.pth'  # The path where your model is saved\n","model.load_state_dict(torch.load(model_load_path))\n","\n","model.eval()\n","\n","acc_list = []\n","cost_list = []\n","incorrect_samples = []\n","predictions = []\n","true_labels = []\n","\n","for i, (test_X, test_Y, sample_ids) in enumerate(data_test_loader):\n","    face_Y, dist_Y, mask_Y = [], [], []\n","    for Y_i in test_Y:\n","        underline_idx = Y_i.find(\"_\")\n","        face_Y.append(int(Y_i[underline_idx-1]))\n","        dist_Y.append(int(Y_i[underline_idx+1]))\n","        mask_Y.append(int(Y_i[underline_idx+3]))\n","\n","    X = test_X.to(device)\n","    face_Y = torch.LongTensor(face_Y).to(device)\n","    dist_Y = torch.LongTensor(dist_Y).to(device)\n","    mask_Y = torch.LongTensor(mask_Y).to(device)\n","\n","    with torch.no_grad():\n","        output = model(X)\n","\n","        cost_face = criterion(output[0], face_Y)\n","        cost_dist = criterion(output[1], dist_Y)\n","        cost_mask = criterion(output[2], mask_Y)\n","        cost = cost_face - 0.015 * cost_dist - 0.01 * cost_mask\n","\n","        accuracy = (torch.max(output[0], 1)[1] == face_Y).float().mean().item()\n","\n","        acc_list.append(accuracy)\n","        cost_list.append(cost.item())\n","\n","        predictions.extend(torch.max(output[0], 1)[1].cpu().numpy())\n","        true_labels.extend(face_Y.cpu().numpy())\n","\n","        print(f'Batch {i} averaged accuracy: {accuracy*100:.2f} %')\n","\n","        incorrect_predictions = (torch.max(output[0], 1)[1] != face_Y)\n","        incorrect_indices = [i for i, x in enumerate(incorrect_predictions) if x]\n","        incorrect_samples.extend([sample_ids[idx] for idx in incorrect_indices])\n","\n","if acc_list:  # Check if acc_list is not empty\n","    print('\\nOverall Accuracy: {:2.2f} %'.format(np.mean(acc_list) * 100))\n","else:\n","    raise Exception(\"\\nNo valid accuracy computations were performed.\")\n","\n","if cost_list:  # Check if cost_list is not empty\n","    print('Overall Cost: {:2.2f}'.format(np.mean(cost_list)))\n","else:\n","    raise Exception(\"\\nNo valid cost computations were performed.\")\n","\n","N = 10  # Specify the number of portions\n","\n","# Initialize vectors to store metrics for each portion\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","# Splitting the data into N portions and calculating metrics\n","portion_size = len(predictions) // N\n","for i in range(N):\n","    start_index = i * portion_size\n","    if i == N - 1:\n","        end_index = len(predictions)  # Ensure to include all elements in the last portion\n","    else:\n","        end_index = start_index + portion_size\n","\n","    portion_predictions = predictions[start_index:end_index]\n","    portion_true_labels = true_labels[start_index:end_index]\n","\n","    # Calculating and storing metrics for the current portion\n","    precisions.append(precision_score(portion_true_labels, portion_predictions))\n","    recalls.append(recall_score(portion_true_labels, portion_predictions))\n","    f1_scores.append(f1_score(portion_true_labels, portion_predictions))\n","\n","# Calculating averaged and median values for each metric\n","avg_precision = np.mean(precisions)\n","median_precision = np.median(precisions)\n","avg_recall = np.mean(recalls)\n","median_recall = np.median(recalls)\n","avg_f1 = np.mean(f1_scores)\n","median_f1 = np.median(f1_scores)\n","\n","# Printing the results\n","print(\"Precision - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_precision * 100, median_precision * 100))\n","print(\"Recall - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_recall * 100, median_recall * 100))\n","print(\"F1-score - Avg: {:.2f}%, Median: {:.2f}%\".format(avg_f1 * 100, median_f1 * 100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YYNZy8g3cmc9","executionInfo":{"status":"ok","timestamp":1711421601909,"user_tz":-480,"elapsed":75039,"user":{"displayName":"Yanbo Zhang","userId":"04052588213183256200"}},"outputId":"36a19889-ba36-4ff2-bd07-92409384d6aa"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 0 averaged accuracy: 96.09 %\n","Batch 1 averaged accuracy: 96.88 %\n","Batch 2 averaged accuracy: 97.66 %\n","Batch 3 averaged accuracy: 96.88 %\n","Batch 4 averaged accuracy: 93.75 %\n","Batch 5 averaged accuracy: 97.66 %\n","Batch 6 averaged accuracy: 96.88 %\n","Batch 7 averaged accuracy: 94.53 %\n","Batch 8 averaged accuracy: 98.44 %\n","Batch 9 averaged accuracy: 96.09 %\n","Batch 10 averaged accuracy: 92.97 %\n","Batch 11 averaged accuracy: 95.31 %\n","Batch 12 averaged accuracy: 96.09 %\n","Batch 13 averaged accuracy: 98.44 %\n","Batch 14 averaged accuracy: 95.31 %\n","Batch 15 averaged accuracy: 95.31 %\n","Batch 16 averaged accuracy: 96.09 %\n","Batch 17 averaged accuracy: 95.31 %\n","Batch 18 averaged accuracy: 98.44 %\n","Batch 19 averaged accuracy: 98.44 %\n","Batch 20 averaged accuracy: 94.53 %\n","Batch 21 averaged accuracy: 96.09 %\n","Batch 22 averaged accuracy: 96.09 %\n","Batch 23 averaged accuracy: 96.09 %\n","Batch 24 averaged accuracy: 95.31 %\n","Batch 25 averaged accuracy: 96.09 %\n","Batch 26 averaged accuracy: 96.88 %\n","Batch 27 averaged accuracy: 96.88 %\n","Batch 28 averaged accuracy: 93.75 %\n","Batch 29 averaged accuracy: 97.66 %\n","Batch 30 averaged accuracy: 98.44 %\n","Batch 31 averaged accuracy: 97.66 %\n","Batch 32 averaged accuracy: 96.09 %\n","Batch 33 averaged accuracy: 95.31 %\n","Batch 34 averaged accuracy: 95.31 %\n","Batch 35 averaged accuracy: 92.97 %\n","Batch 36 averaged accuracy: 96.88 %\n","Batch 37 averaged accuracy: 96.09 %\n","Batch 38 averaged accuracy: 98.44 %\n","Batch 39 averaged accuracy: 95.31 %\n","Batch 40 averaged accuracy: 95.31 %\n","Batch 41 averaged accuracy: 96.88 %\n","Batch 42 averaged accuracy: 95.31 %\n","Batch 43 averaged accuracy: 94.53 %\n","Batch 44 averaged accuracy: 97.66 %\n","Batch 45 averaged accuracy: 96.88 %\n","Batch 46 averaged accuracy: 94.53 %\n","Batch 47 averaged accuracy: 97.66 %\n","Batch 48 averaged accuracy: 93.75 %\n","Batch 49 averaged accuracy: 100.00 %\n","Batch 50 averaged accuracy: 95.31 %\n","Batch 51 averaged accuracy: 96.88 %\n","Batch 52 averaged accuracy: 97.66 %\n","Batch 53 averaged accuracy: 96.09 %\n","Batch 54 averaged accuracy: 96.88 %\n","Batch 55 averaged accuracy: 96.88 %\n","Batch 56 averaged accuracy: 93.75 %\n","Batch 57 averaged accuracy: 96.09 %\n","Batch 58 averaged accuracy: 96.09 %\n","Batch 59 averaged accuracy: 96.88 %\n","Batch 60 averaged accuracy: 96.88 %\n","Batch 61 averaged accuracy: 97.66 %\n","Batch 62 averaged accuracy: 96.88 %\n","Batch 63 averaged accuracy: 96.09 %\n","Batch 64 averaged accuracy: 97.66 %\n","Batch 65 averaged accuracy: 96.09 %\n","Batch 66 averaged accuracy: 96.09 %\n","Batch 67 averaged accuracy: 94.53 %\n","Batch 68 averaged accuracy: 97.66 %\n","Batch 69 averaged accuracy: 97.66 %\n","Batch 70 averaged accuracy: 95.31 %\n","Batch 71 averaged accuracy: 95.31 %\n","Batch 72 averaged accuracy: 96.88 %\n","Batch 73 averaged accuracy: 96.09 %\n","Batch 74 averaged accuracy: 97.66 %\n","Batch 75 averaged accuracy: 96.88 %\n","Batch 76 averaged accuracy: 96.88 %\n","Batch 77 averaged accuracy: 97.66 %\n","Batch 78 averaged accuracy: 93.75 %\n","Batch 79 averaged accuracy: 97.66 %\n","Batch 80 averaged accuracy: 94.53 %\n","Batch 81 averaged accuracy: 95.31 %\n","Batch 82 averaged accuracy: 97.66 %\n","Batch 83 averaged accuracy: 98.44 %\n","Batch 84 averaged accuracy: 96.09 %\n","Batch 85 averaged accuracy: 99.22 %\n","Batch 86 averaged accuracy: 97.66 %\n","Batch 87 averaged accuracy: 92.97 %\n","Batch 88 averaged accuracy: 96.09 %\n","Batch 89 averaged accuracy: 97.66 %\n","Batch 90 averaged accuracy: 93.75 %\n","Batch 91 averaged accuracy: 97.66 %\n","Batch 92 averaged accuracy: 95.31 %\n","Batch 93 averaged accuracy: 96.09 %\n","Batch 94 averaged accuracy: 100.00 %\n","Batch 95 averaged accuracy: 96.88 %\n","Batch 96 averaged accuracy: 97.66 %\n","Batch 97 averaged accuracy: 99.22 %\n","Batch 98 averaged accuracy: 96.09 %\n","Batch 99 averaged accuracy: 96.09 %\n","Batch 100 averaged accuracy: 94.53 %\n","Batch 101 averaged accuracy: 96.88 %\n","Batch 102 averaged accuracy: 94.53 %\n","Batch 103 averaged accuracy: 96.88 %\n","Batch 104 averaged accuracy: 96.09 %\n","Batch 105 averaged accuracy: 97.66 %\n","Batch 106 averaged accuracy: 98.44 %\n","Batch 107 averaged accuracy: 95.31 %\n","Batch 108 averaged accuracy: 95.31 %\n","Batch 109 averaged accuracy: 96.09 %\n","Batch 110 averaged accuracy: 99.22 %\n","Batch 111 averaged accuracy: 97.66 %\n","Batch 112 averaged accuracy: 96.09 %\n","Batch 113 averaged accuracy: 98.44 %\n","Batch 114 averaged accuracy: 96.88 %\n","Batch 115 averaged accuracy: 93.75 %\n","Batch 116 averaged accuracy: 91.41 %\n","Batch 117 averaged accuracy: 97.66 %\n","Batch 118 averaged accuracy: 94.53 %\n","Batch 119 averaged accuracy: 96.88 %\n","Batch 120 averaged accuracy: 94.53 %\n","Batch 121 averaged accuracy: 96.09 %\n","Batch 122 averaged accuracy: 93.75 %\n","Batch 123 averaged accuracy: 98.44 %\n","Batch 124 averaged accuracy: 94.53 %\n","Batch 125 averaged accuracy: 93.75 %\n","Batch 126 averaged accuracy: 94.53 %\n","Batch 127 averaged accuracy: 94.53 %\n","Batch 128 averaged accuracy: 96.09 %\n","Batch 129 averaged accuracy: 95.31 %\n","Batch 130 averaged accuracy: 94.53 %\n","Batch 131 averaged accuracy: 95.31 %\n","Batch 132 averaged accuracy: 98.44 %\n","Batch 133 averaged accuracy: 96.09 %\n","Batch 134 averaged accuracy: 96.88 %\n","Batch 135 averaged accuracy: 96.09 %\n","Batch 136 averaged accuracy: 96.09 %\n","Batch 137 averaged accuracy: 96.09 %\n","Batch 138 averaged accuracy: 98.44 %\n","Batch 139 averaged accuracy: 96.88 %\n","Batch 140 averaged accuracy: 93.75 %\n","Batch 141 averaged accuracy: 95.31 %\n","Batch 142 averaged accuracy: 98.44 %\n","Batch 143 averaged accuracy: 94.53 %\n","Batch 144 averaged accuracy: 96.88 %\n","Batch 145 averaged accuracy: 96.88 %\n","Batch 146 averaged accuracy: 93.75 %\n","Batch 147 averaged accuracy: 98.44 %\n","Batch 148 averaged accuracy: 93.75 %\n","Batch 149 averaged accuracy: 95.31 %\n","Batch 150 averaged accuracy: 99.22 %\n","Batch 151 averaged accuracy: 96.88 %\n","Batch 152 averaged accuracy: 96.09 %\n","Batch 153 averaged accuracy: 96.09 %\n","Batch 154 averaged accuracy: 94.53 %\n","Batch 155 averaged accuracy: 96.09 %\n","Batch 156 averaged accuracy: 95.31 %\n","Batch 157 averaged accuracy: 96.88 %\n","Batch 158 averaged accuracy: 94.53 %\n","Batch 159 averaged accuracy: 97.66 %\n","Batch 160 averaged accuracy: 95.31 %\n","Batch 161 averaged accuracy: 96.88 %\n","Batch 162 averaged accuracy: 95.31 %\n","Batch 163 averaged accuracy: 97.66 %\n","Batch 164 averaged accuracy: 96.88 %\n","Batch 165 averaged accuracy: 97.66 %\n","Batch 166 averaged accuracy: 94.53 %\n","Batch 167 averaged accuracy: 96.88 %\n","Batch 168 averaged accuracy: 98.44 %\n","Batch 169 averaged accuracy: 95.31 %\n","Batch 170 averaged accuracy: 96.88 %\n","Batch 171 averaged accuracy: 93.75 %\n","Batch 172 averaged accuracy: 92.97 %\n","Batch 173 averaged accuracy: 93.75 %\n","Batch 174 averaged accuracy: 96.88 %\n","Batch 175 averaged accuracy: 96.88 %\n","Batch 176 averaged accuracy: 92.97 %\n","Batch 177 averaged accuracy: 96.09 %\n","Batch 178 averaged accuracy: 96.09 %\n","Batch 179 averaged accuracy: 93.75 %\n","Batch 180 averaged accuracy: 93.75 %\n","Batch 181 averaged accuracy: 96.88 %\n","Batch 182 averaged accuracy: 96.09 %\n","Batch 183 averaged accuracy: 96.88 %\n","Batch 184 averaged accuracy: 95.31 %\n","Batch 185 averaged accuracy: 95.31 %\n","Batch 186 averaged accuracy: 95.31 %\n","Batch 187 averaged accuracy: 98.44 %\n","Batch 188 averaged accuracy: 92.19 %\n","Batch 189 averaged accuracy: 97.66 %\n","Batch 190 averaged accuracy: 96.09 %\n","Batch 191 averaged accuracy: 95.31 %\n","Batch 192 averaged accuracy: 99.22 %\n","Batch 193 averaged accuracy: 94.53 %\n","Batch 194 averaged accuracy: 96.09 %\n","Batch 195 averaged accuracy: 96.88 %\n","Batch 196 averaged accuracy: 94.53 %\n","Batch 197 averaged accuracy: 96.88 %\n","Batch 198 averaged accuracy: 96.88 %\n","Batch 199 averaged accuracy: 96.09 %\n","Batch 200 averaged accuracy: 96.09 %\n","Batch 201 averaged accuracy: 97.66 %\n","Batch 202 averaged accuracy: 97.66 %\n","Batch 203 averaged accuracy: 97.66 %\n","Batch 204 averaged accuracy: 96.09 %\n","Batch 205 averaged accuracy: 99.22 %\n","Batch 206 averaged accuracy: 94.53 %\n","Batch 207 averaged accuracy: 96.88 %\n","Batch 208 averaged accuracy: 94.53 %\n","Batch 209 averaged accuracy: 92.97 %\n","Batch 210 averaged accuracy: 95.83 %\n","\n","Overall Accuracy: 96.18 %\n","Overall Cost: 0.34\n","Precision - Avg: 93.32%, Median: 93.43%\n","Recall - Avg: 100.00%, Median: 100.00%\n","F1-score - Avg: 96.54%, Median: 96.60%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1Uostja2Or2NrNyRaPFV9lS_DF7JZUuaw","authorship_tag":"ABX9TyPW07NGol6UloVoyAln/zJY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
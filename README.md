# AcFace

## Introduction of this project
This project develops an acoustic-based facial recognition system, which is optimized for robust performance even in situations where facial masks obstruct visibility. 
The system operates through two key components: a signal processing module that translates raw acoustic signals into explicit 3D facial representations, and a deep learning model for recognition and discrimination, ensuring high accuracy even when facial masks are present. The figure below illustrates the system structure. 

In this repository, we provide a full-stack implementation of the system, comprising the signal processing code for extracting facial spectrums and the implementation of the deep learning model for spectrum recognition. Additionally, this artifact includes a self-constructed dataset, enabling evaluation of system performance across various usage scenarios.

![AcFace system structure.](https://github.com/yanbozhang003/AcFace-AE/blob/main/AcFace_structure.png)

<img src="https://github.com/yanbozhang003/AcFace-AE/blob/main/AcFace_structure.png" alt="Acface system structure" width="800" height="275"/>

<div style="text-align: center;">
    <img src="https://github.com/yanbozhang003/AcFace-AE/blob/main/AcFace_structure.png" alt="Acface system structure" width="300" height="200"/>
</div>

## Code structure

## Hardware dependencies

## Usage

### Audio signal generation and capture

### Facial Spectrum

### RD-Net 

#### Training

#### Evaluation

##### Accuracy, Precision, Recall, F1-score

##### Scalability

##### Efficiency

##### Parameter settings


